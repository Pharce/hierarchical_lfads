{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from models import LFADS\n",
    "from utils import read_data, load_parameters, save_parameters, batchify_random_sample\n",
    "\n",
    "np = torch._np\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'; print('Using device: %s'%device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/anaconda2/envs/pytorch04/lib/python3.5/site-packages/oasis/functions.py:14: UserWarning: Could not find cvxpy. Don't worry, you can still use OASIS, just not the slower interior point methods we compared to in the papers.\n",
      "  \"just not the slower interior point methods we compared to in the papers.\")\n",
      "/home/luke/anaconda2/envs/pytorch04/lib/python3.5/site-packages/scipy/stats/_binned_statistic.py:607: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  result = result[core]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving variable with name:  valid_latent\n",
      "Saving variable with name:  dt\n",
      "Saving variable with name:  train_latent\n",
      "Saving variable with name:  valid_oasis\n",
      "Saving variable with name:  train_calcium\n",
      "Saving variable with name:  valid_spikes\n",
      "Saving variable with name:  train_data\n",
      "Saving variable with name:  valid_data\n",
      "Saving variable with name:  train_oasis\n",
      "Saving variable with name:  conversion_factor\n",
      "Saving variable with name:  train_truth\n",
      "Saving variable with name:  valid_rates\n",
      "Saving variable with name:  train_fluor\n",
      "Saving variable with name:  valid_truth\n",
      "Saving variable with name:  valid_fluor\n",
      "Saving variable with name:  train_rates\n",
      "Saving variable with name:  valid_calcium\n",
      "Saving variable with name:  train_spikes\n",
      "Saving variable with name:  loading_weights\n",
      "torch.Size([1040, 100, 30])\n",
      "Number of datapoints = 3120000\n"
     ]
    }
   ],
   "source": [
    "author = 'lyprince'\n",
    "seed = 100\n",
    "if os.path.exists('./synth_data/lorenz_100'):\n",
    "    data_dict = read_data('./synth_data/lorenz_100')\n",
    "else:\n",
    "    from synthetic_data import generate_lorenz_data\n",
    "    data_dict = generate_lorenz_data(N_cells=30, N_inits=65, N_trials=20, N_steps=200, N_stepsinbin=2, dt_lorenz=0.015, dt_spike = 1./20, base_firing_rate= 1.0, save=True)\n",
    "\n",
    "# For spike data\n",
    "train_data = torch.Tensor(data_dict['train_spikes']).to(device)\n",
    "valid_data = torch.Tensor(data_dict['valid_spikes']).to(device)\n",
    "\n",
    "train_truth = {'rates'  : data_dict['train_rates'],\n",
    "               'latent' : data_dict['train_latent']}\n",
    "\n",
    "valid_truth = {'rates'  : data_dict['valid_rates'],\n",
    "               'latent' : data_dict['valid_latent']}\n",
    "\n",
    "train_ds      = torch.utils.data.TensorDataset(train_data)\n",
    "valid_ds      = torch.utils.data.TensorDataset(valid_data)\n",
    "\n",
    "num_trials, num_steps, num_cells = train_data.shape;\n",
    "print(train_data.shape);\n",
    "print('Number of datapoints = %s'%train_data.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'betas': (0.9, 0.99),\n",
       " 'clip_val': 5.0,\n",
       " 'dataset_name': 'lorenz',\n",
       " 'datatype': 'spikes',\n",
       " 'epsilon': 0.1,\n",
       " 'factors_dim': 3,\n",
       " 'g0_encoder_dim': 64,\n",
       " 'g0_prior_kappa': 0.1,\n",
       " 'g0_prior_var_max': 0.1,\n",
       " 'g0_prior_var_min': 0.1,\n",
       " 'g_dim': 64,\n",
       " 'keep_prob': 0.95,\n",
       " 'kernel_dim': 20,\n",
       " 'kl_weight_min': 0.0,\n",
       " 'kl_weight_schedule_dur': 1600,\n",
       " 'kl_weight_schedule_start': 0,\n",
       " 'l2_con_scale': 0,\n",
       " 'l2_gen_scale': 250,\n",
       " 'l2_weight_min': 0.0,\n",
       " 'l2_weight_schedule_dur': 1600,\n",
       " 'l2_weight_schedule_start': 0.0,\n",
       " 'learning_rate': 0.01,\n",
       " 'learning_rate_decay': 0.95,\n",
       " 'learning_rate_min': 1e-05,\n",
       " 'max_norm': 200,\n",
       " 'norm_factors': True,\n",
       " 'run_name': 'poisson_demo',\n",
       " 'scheduler_cooldown': 6,\n",
       " 'scheduler_on': True,\n",
       " 'scheduler_patience': 6,\n",
       " 'u_dim': 0,\n",
       " 'u_prior_kappa': 0.1,\n",
       " 'use_weight_schedule_fn': True}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams = load_parameters('./parameters/parameters_lorenz_spikes.yaml')\n",
    "hyperparams['run_name'] += '_demo'\n",
    "save_parameters(hyperparams, path=None)\n",
    "\n",
    "hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed: 7968\n"
     ]
    }
   ],
   "source": [
    "model = LFADS(inputs_dim = num_cells, T = num_steps, dt = float(data_dict['dt']), device=device,\n",
    "              model_hyperparams=hyperparams).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training...\n",
      "Epoch:    1, Step:    16, training loss: 1289.844, validation loss: 1478.769\n",
      "Epoch:    2, Step:    32, training loss: 1273.732, validation loss: 1387.469\n",
      "Epoch:    3, Step:    48, training loss: 1263.496, validation loss: 1283.039\n",
      "Epoch:    4, Step:    64, training loss: 1257.180, validation loss: 1270.510\n",
      "Epoch:    5, Step:    80, training loss: 1253.778, validation loss: 1269.829\n",
      "Epoch:    6, Step:    96, training loss: 1250.417, validation loss: 1272.616\n",
      "Epoch:    7, Step:   112, training loss: 1247.252, validation loss: 1283.232\n",
      "Epoch:    8, Step:   128, training loss: 1243.691, validation loss: 1274.111\n",
      "Epoch:    9, Step:   144, training loss: 1237.998, validation loss: 1270.269\n",
      "Epoch:   10, Step:   160, training loss: 1236.176, validation loss: 1266.048\n",
      "Epoch:   11, Step:   176, training loss: 1231.137, validation loss: 1259.896\n",
      "Epoch:   12, Step:   192, training loss: 1227.174, validation loss: 1260.505\n",
      "Epoch:   13, Step:   208, training loss: 1221.722, validation loss: 1253.010\n",
      "Epoch:   14, Step:   224, training loss: 1220.357, validation loss: 1257.200\n",
      "Epoch:   15, Step:   240, training loss: 1218.540, validation loss: 1248.313\n",
      "Epoch:   16, Step:   256, training loss: 1215.600, validation loss: 1241.461\n",
      "Epoch:   17, Step:   272, training loss: 1207.884, validation loss: 1240.252\n",
      "Epoch:   18, Step:   288, training loss: 1207.182, validation loss: 1233.698\n",
      "Epoch:   19, Step:   304, training loss: 1203.106, validation loss: 1233.167\n",
      "Epoch:   20, Step:   320, training loss: 1198.851, validation loss: 1225.317\n",
      "Epoch:   21, Step:   336, training loss: 1194.734, validation loss: 1227.716\n",
      "Epoch:   22, Step:   352, training loss: 1194.717, validation loss: 1235.439\n",
      "Epoch:   23, Step:   368, training loss: 1193.272, validation loss: 1221.371\n",
      "Epoch:   24, Step:   384, training loss: 1188.156, validation loss: 1219.915\n",
      "Epoch:   25, Step:   400, training loss: 1187.221, validation loss: 1211.107\n",
      "Epoch:   26, Step:   416, training loss: 1186.805, validation loss: 1210.492\n",
      "Epoch:   27, Step:   432, training loss: 1186.678, validation loss: 1203.852\n",
      "Epoch:   28, Step:   448, training loss: 1180.745, validation loss: 1208.452\n",
      "Epoch:   29, Step:   464, training loss: 1188.359, validation loss: 1206.113\n",
      "Epoch:   30, Step:   480, training loss: 1187.216, validation loss: 1205.717\n",
      "Epoch:   31, Step:   496, training loss: 1179.102, validation loss: 1208.822\n",
      "Epoch:   32, Step:   512, training loss: 1177.193, validation loss: 1193.402\n",
      "Epoch:   33, Step:   528, training loss: 1179.431, validation loss: 1197.735\n",
      "Epoch:   34, Step:   544, training loss: 1178.480, validation loss: 1199.972\n",
      "Epoch:   35, Step:   560, training loss: 1180.960, validation loss: 1199.321\n",
      "Epoch:   36, Step:   576, training loss: 1176.660, validation loss: 1190.394\n",
      "Epoch:   37, Step:   592, training loss: 1174.052, validation loss: 1192.240\n",
      "Epoch:   38, Step:   608, training loss: 1170.589, validation loss: 1189.476\n",
      "Epoch:   39, Step:   624, training loss: 1174.222, validation loss: 1196.408\n",
      "Epoch:   40, Step:   640, training loss: 1170.805, validation loss: 1184.343\n",
      "Epoch:   41, Step:   656, training loss: 1170.159, validation loss: 1189.860\n",
      "Epoch:   42, Step:   672, training loss: 1170.096, validation loss: 1183.994\n",
      "Epoch:   43, Step:   688, training loss: 1172.012, validation loss: 1181.563\n",
      "Epoch:   44, Step:   704, training loss: 1171.632, validation loss: 1187.895\n",
      "Epoch:   45, Step:   720, training loss: 1167.700, validation loss: 1178.275\n",
      "Epoch:   46, Step:   736, training loss: 1165.528, validation loss: 1183.502\n",
      "Epoch:   47, Step:   752, training loss: 1168.297, validation loss: 1176.947\n",
      "Epoch:   48, Step:   768, training loss: 1167.019, validation loss: 1178.840\n",
      "Epoch:   49, Step:   784, training loss: 1174.848, validation loss: 1182.359\n",
      "Learning rate decreased to 0.009500\n",
      "Epoch:   50, Step:   800, training loss: 1173.240, validation loss: 1176.147\n",
      "Epoch:   51, Step:   816, training loss: 1167.250, validation loss: 1174.895\n",
      "Epoch:   52, Step:   832, training loss: 1161.437, validation loss: 1172.202\n",
      "Epoch:   53, Step:   848, training loss: 1161.693, validation loss: 1178.262\n",
      "Epoch:   54, Step:   864, training loss: 1169.968, validation loss: 1180.496\n",
      "Epoch:   55, Step:   880, training loss: 1167.254, validation loss: 1179.130\n",
      "Epoch:   56, Step:   896, training loss: 1163.345, validation loss: 1173.971\n",
      "Epoch:   57, Step:   912, training loss: 1166.549, validation loss: 1182.479\n",
      "Epoch:   58, Step:   928, training loss: 1169.873, validation loss: 1178.541\n",
      "Epoch:   59, Step:   944, training loss: 1166.183, validation loss: 1176.840\n",
      "Epoch:   60, Step:   960, training loss: 1164.805, validation loss: 1174.311\n",
      "Epoch:   61, Step:   976, training loss: 1162.643, validation loss: 1176.195\n",
      "Epoch:   62, Step:   992, training loss: 1162.305, validation loss: 1169.866\n",
      "Epoch:   63, Step:  1008, training loss: 1159.224, validation loss: 1165.810\n",
      "Epoch:   64, Step:  1024, training loss: 1158.969, validation loss: 1168.406\n",
      "Epoch:   65, Step:  1040, training loss: 1160.311, validation loss: 1165.064\n",
      "Epoch:   66, Step:  1056, training loss: 1162.491, validation loss: 1167.787\n",
      "Epoch:   67, Step:  1072, training loss: 1162.077, validation loss: 1181.669\n",
      "Epoch:   68, Step:  1088, training loss: 1166.384, validation loss: 1166.331\n",
      "Learning rate decreased to 0.009025\n",
      "Epoch:   69, Step:  1104, training loss: 1160.842, validation loss: 1173.790\n",
      "Epoch:   70, Step:  1120, training loss: 1161.299, validation loss: 1164.423\n",
      "Epoch:   71, Step:  1136, training loss: 1162.529, validation loss: 1167.471\n",
      "Epoch:   72, Step:  1152, training loss: 1160.430, validation loss: 1162.268\n",
      "Epoch:   73, Step:  1168, training loss: 1157.833, validation loss: 1167.352\n",
      "Epoch:   74, Step:  1184, training loss: 1159.570, validation loss: 1166.611\n",
      "Epoch:   75, Step:  1200, training loss: 1158.556, validation loss: 1167.049\n",
      "Epoch:   76, Step:  1216, training loss: 1159.980, validation loss: 1162.617\n",
      "Epoch:   77, Step:  1232, training loss: 1162.346, validation loss: 1177.148\n",
      "Epoch:   78, Step:  1248, training loss: 1166.311, validation loss: 1167.774\n",
      "Learning rate decreased to 0.008574\n",
      "Epoch:   79, Step:  1264, training loss: 1161.444, validation loss: 1162.431\n",
      "Epoch:   80, Step:  1280, training loss: 1162.766, validation loss: 1164.918\n",
      "Epoch:   81, Step:  1296, training loss: 1159.307, validation loss: 1158.657\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_ds, valid_ds, train_truth=train_truth, valid_truth=valid_truth,\n",
    "          max_epochs=2000, batch_size=65, use_tensorboard=True, health_check=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
