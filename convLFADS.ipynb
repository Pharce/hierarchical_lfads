{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do:\n",
    "\n",
    "# 2- play with hyper-parameters, and loss\n",
    "# 3- check on test error\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from lfads import LFADS_Net\n",
    "from objective import *\n",
    "from scheduler import LFADS_Scheduler\n",
    "\n",
    "\n",
    "class conv_block(nn.Module):# *args, **kwargs \n",
    "    def __init__(self, in_f, out_f):\n",
    "        super(conv_block,self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv3d(in_f, out_f, \n",
    "                  kernel_size=3, \n",
    "                  padding=1)\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=(1,2,2),\n",
    "                     return_indices=True)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x, ind = self.pool1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        return x, ind\n",
    "        \n",
    "\n",
    "\n",
    "class deconv_block(nn.Module):\n",
    "    def __init__(self, in_f, out_f):\n",
    "        super(deconv_block,self).__init__()\n",
    "        \n",
    "        self.unpool1 = nn.MaxUnpool3d(kernel_size=(1,2,2))\n",
    "        \n",
    "        self.deconv1 = nn.ConvTranspose3d(in_channels=in_f,\n",
    "                                          out_channels=out_f,\n",
    "                                          kernel_size=3,\n",
    "                                          padding=1, \n",
    "                                         )\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "    def forward(self,x,ind):\n",
    "        \n",
    "        x = self.unpool1(x,ind)\n",
    "        x = self.deconv1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class convVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(convVAE,self).__init__()\n",
    "        \n",
    "        in_f = 1\n",
    "        out_f = [2,3]\n",
    "        all_f = [in_f,*out_f]\n",
    "        self.n_layers = 2\n",
    "        \n",
    "        self.video_dim_space = 128\n",
    "        self.video_dim_time = 50\n",
    "        self.final_size = 32\n",
    "        self.final_f = 3\n",
    "        \n",
    "        self.convlayers = nn.ModuleList()\n",
    "        for n in range(0,self.n_layers):\n",
    "            self.convlayers.add_module('{}{}'.format('ce', n),conv_block(all_f[n], all_f[n+1]))\n",
    "#         self.convlayers.add_module('ce1',conv_block(out_f1, out_f2))\n",
    "        \n",
    "        self.deconvlayers = nn.ModuleList()\n",
    "        for n in range(0,self.n_layers):\n",
    "            self.deconvlayers.add_module('{}{}'.format('dec', n),deconv_block(all_f[self.n_layers-n], all_f[self.n_layers-n-1]))\n",
    "#         self.deconvlayers.add_module('dec0',deconv_block(out_f2,out_f1))\n",
    "#         self.deconvlayers.add_module('dec1',deconv_block(out_f1,in_f))\n",
    "#         self.ce1 = conv_block(in_f, out_f1) \n",
    "#         self.ce2 = conv_block(out_f1, out_f2)\n",
    "\n",
    "#         self.dec1 = deconv_block(out_f2,out_f1)\n",
    "#         self.dec2 = deconv_block(out_f1,in_f) \n",
    "\n",
    "        self.lfads = LFADS_Net(self.final_size * self.final_size * self.final_f, output_size = None, factor_size = 4,\n",
    "                 g_encoder_size  = 64, c_encoder_size = 64,\n",
    "                 g_latent_size   = 64, u_latent_size  = 1,\n",
    "                 controller_size = 64, generator_size = 64,\n",
    "                 prior = {'g0' : {'mean' : {'value': 0.0, 'learnable' : True},\n",
    "                                  'var'  : {'value': 0.1, 'learnable' : False}},\n",
    "                          'u'  : {'mean' : {'value': 0.0, 'learnable' : False},\n",
    "                                  'var'  : {'value': 0.1, 'learnable' : True},\n",
    "                                  'tau'  : {'value': 10,  'learnable' : True}}},\n",
    "                 clip_val=5.0, dropout=0.0, max_norm = 200, deep_freeze = False,\n",
    "                 do_normalize_factors=True, device='cpu')\n",
    "\n",
    "        \n",
    "    def forward(self,video):\n",
    "        x = video\n",
    "        Ind = list()\n",
    "        for n, layer in enumerate(self.convlayers):\n",
    "            x, ind1 = layer(x)\n",
    "            Ind.append(ind1)\n",
    "        \n",
    "        x = x.permute(0,2,1,3,4)\n",
    "        x = x.reshape(x.shape[0],x.shape[1],-1)\n",
    "        \n",
    "        x = x.permute(1,0,2)\n",
    "        r,_ = self.lfads(x)\n",
    "        x = r['data']\n",
    "        x = x.permute(1,0,2)\n",
    "        # call LFADS here:\n",
    "        # x should be reshaped for LFADS [time x batch x cells]:\n",
    "        # \n",
    "        # LFADS output should be also reshaped back for the conv decoder\n",
    "        \n",
    "        x = x.reshape(x.shape[0],x.shape[1],self.final_f,self.final_size, self.final_size)\n",
    "        x = x.permute(0,2,1,3,4)\n",
    "\n",
    "        \n",
    "        \n",
    "        for n, layer in enumerate(self.deconvlayers):     \n",
    "            x = layer(x,Ind[self.n_layers-n-1])\n",
    "            \n",
    "\n",
    "#         x, ind1 = self.ce0(video)\n",
    "#         x, ind2 = self.ce1(x)\n",
    "#         x = self.dec0(x,ind2)\n",
    "#         v_p = self.dec1(x,ind1)\n",
    "        \n",
    "\n",
    "#         return v_p\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    \n",
    "    from synthetic_data import generate_lorenz_data, SyntheticCalciumVideoDataset\n",
    "\n",
    "    # convert data to torch.FloatTensor\n",
    "    transform = transforms.ToTensor()\n",
    "\n",
    "    # load the training and test datasets\n",
    "\n",
    "    data_dict = generate_lorenz_data(20, 65, 50, 50, save=False)\n",
    "    cells = data_dict['cells']\n",
    "    traces = data_dict['train_fluor']\n",
    "    train_data = SyntheticCalciumVideoDataset(traces=traces, cells=cells)\n",
    "    test_data = SyntheticCalciumVideoDataset(traces=traces, cells=cells)\n",
    "    \n",
    "    num_workers = 0\n",
    "    # how many samples per batch to load\n",
    "    batch_size = 20\n",
    "\n",
    "    # prepare data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    return train_data, train_loader, test_loader\n",
    "\n",
    "class convLFADS_loss(nn.Module):\n",
    "    def __init__(self,\n",
    "                 kl_weight_init=0.1, l2_weight_init=0.1,\n",
    "                 kl_weight_schedule_dur = 2000, l2_weight_schedule_dur = 2000,\n",
    "                 kl_weight_schedule_start = 0, l2_weight_schedule_start = 0,\n",
    "                 kl_weight_max=1.0, l2_weight_max=1.0,\n",
    "                 l2_con_scale=1.0, l2_gen_scale=1.0):\n",
    "        super(convLFADS_loss,self).__init__()\n",
    "        \n",
    "        self.loss_weights = {'kl' : {'weight' : kl_weight_init,\n",
    "                                     'schedule_dur' : kl_weight_schedule_dur,\n",
    "                                     'schedule_start' : kl_weight_schedule_start,\n",
    "                                     'max' : kl_weight_max,\n",
    "                                     'min' : kl_weight_init},\n",
    "                             'l2' : {'weight' : l2_weight_init,\n",
    "                                     'schedule_dur' : l2_weight_schedule_dur,\n",
    "                                     'schedule_start' : l2_weight_schedule_start,\n",
    "                                     'max' : l2_weight_max,\n",
    "                                     'min' : l2_weight_init}}\n",
    "        self.l2_con_scale = l2_con_scale\n",
    "        self.l2_gen_scale = l2_gen_scale\n",
    "        self.recon_loss = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, video_orig, video_recon, model):\n",
    "        kl_weight = self.loss_weights['kl']['weight']\n",
    "        l2_weight = self.loss_weights['l2']['weight']\n",
    "        \n",
    "#         recon_loss = -self.loglikelihood(x_orig.permute(1, 0, 2), x_recon['data'].permute(1, 0, 2))\n",
    "        recon_loss = self.recon_loss(video_recon,video_orig)\n",
    "\n",
    "        kl_loss = kl_weight * kldiv_gaussian_gaussian(post_mu  = model.g_posterior_mean,\n",
    "                                                      post_lv  = model.g_posterior_logvar,\n",
    "                                                      prior_mu = model.g_prior_mean,\n",
    "                                                      prior_lv = model.g_prior_logvar)\n",
    "    \n",
    "        l2_loss = 0.5 * l2_weight * self.l2_gen_scale * model.generator.gru_generator.hidden_weight_l2_norm()\n",
    "    \n",
    "#         if hasattr(model, 'controller'):\n",
    "#             kl_loss += kl_weight * kldiv_gaussian_gaussian(post_mu  = model.u_posterior_mean,\n",
    "#                                                            post_lv  = model.u_posterior_logvar,\n",
    "#                                                            prior_mu = model.u_prior_mean,\n",
    "#                                                            prior_lv = model.u_prior_logvar)\n",
    "            \n",
    "#             l2_loss += 0.5 * l2_weight * self.l2_con_scale * model.controller.gru_controller.hidden_weight_l2_norm()\n",
    "            \n",
    "        return recon_loss, kl_loss, l2_loss\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "def train_convVAE(train_loader,test_loader,n_epochs): #model,\n",
    "    model = convVAE()\n",
    "    lfads = model.lfads\n",
    "    # number of epochs to train the model\n",
    "#     n_epochs = 30\n",
    "#     train_loader, test_loader = get_data()\n",
    "#     model = convVAE()\n",
    "#     criterion = nn.MSELoss()\n",
    "    criterion = convLFADS_loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#     scheduler = LFADS_Scheduler(optimizer      =  optimizer,\n",
    "#                                 mode           =  'min', \n",
    "#                                 factor         =  0.1, \n",
    "#                                 patience       =  10,\n",
    "#                                 verbose        =  False, \n",
    "#                                 threshold      = 1e-4, \n",
    "#                                 threshold_mode = 'rel',\n",
    "#                                 cooldown       =  0, \n",
    "#                                 min_lr         =  0,\n",
    "#                                 eps            =  1e-8)\n",
    "    \n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # monitor training loss\n",
    "        train_loss = 0.0\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        i = 0\n",
    "        for data in train_loader:\n",
    "            \n",
    "#             print(i)\n",
    "            # _ stands in for labels, here\n",
    "            # no need to flatten images\n",
    "            videos = data\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            outputs = model(videos)\n",
    "            # calculate the loss\n",
    "            recon_loss, kl_loss, l2_loss = criterion(outputs, videos,lfads)\n",
    "            loss = recon_loss + kl_loss + l2_loss\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update running training loss\n",
    "            train_loss += loss.item()*videos.size(0)\n",
    "            i += 1\n",
    "            \n",
    "#             scheduler.step(loss)\n",
    "            \n",
    "        # print avg training statistics \n",
    "        train_loss = train_loss/len(train_loader)\n",
    "        print('Epoch: {} \\tTotal Loss: {:.6f} \\tl2 Loss: {:.6f} \\tkl Loss: {:.6f}'.format(\n",
    "            epoch, train_loss, l2_loss, kl_loss))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Lorenz data\n",
      "Converting to rates and spikes\n",
      "Converting to fluorescence\n",
      "Train and test split\n",
      "Saving to .//synth_data/lorenz_100\n",
      "Epoch: 1 \tTotal Loss: 64.086338 \tl2 Loss: 0.001361 \tkl Loss: 0.031413\n",
      "Epoch: 2 \tTotal Loss: 15.824891 \tl2 Loss: 0.001252 \tkl Loss: 0.002298\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0b6dba4829b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_convVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-d63da16bcb40>\u001b[0m in \u001b[0;36mtrain_convVAE\u001b[0;34m(train_loader, test_loader, n_epochs)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m###################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;31m#             print(i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Mila/Project-Codes/Virtual-Environments/cenv-hlfads/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Mila/Project-Codes/Virtual-Environments/cenv-hlfads/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Mila/Project-Codes/Virtual-Environments/cenv-hlfads/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Mila/Project-Codes/hierarchical_lfads/synthetic_data.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, ix)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraces\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcells\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data, train_loader, test_loader = get_data()\n",
    "train_convVAE(train_loader,test_loader,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
