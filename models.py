import os
import datetime
import random

import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as opt
np = torch._np
from torch.autograd import Variable
import matplotlib.pyplot as plt
from utils import batchify_random_sample, update_param_dict

from math import log, exp, pi

import pdb
import time

#---------------------------------------------------
def apply_requires_grad(module, requires_grad=True):
    for p in module.parameters():
        p.requires_grad = requires_grad
        
        
from sklearn.linear_model import LinearRegression

def fit_linear_model(x, y):
    L = LinearRegression().fit(x, y)
    return L

def r_squared(x, y):
    L = fit_linear_model(x, y)
    return L.score(x, y), L

class Identity(nn.Module):
    def __init__(self, in_features, out_features):
        super(Identity, self).__init__()
        self.weight = torch.eye(in_features)
    
    def forward(self, x):
        return x.matmul(self.weight)

#-------------------------
# COST FUNCTION COMPONENTS
#-------------------------

def KLCostGaussian(post_mu, post_lv, prior_mu, prior_lv):
    '''
    KLCostGaussian(post_mu, post_lv, prior_mu, prior_lv)

    KL-Divergence between a prior and posterior diagonal Gaussian distribution.

    Arguments:
        - post_mu (torch.Tensor): mean for the posterior
        - post_lv (torch.Tensor): logvariance for the posterior
        - prior_mu (torch.Tensor): mean for the prior
        - prior_lv (torch.Tensor): logvariance for the prior
    '''
    klc = 0.5 * (prior_lv - post_lv + torch.exp(post_lv - prior_lv) \
         + ((post_mu - prior_mu)/torch.exp(0.5 * prior_lv)).pow(2) - 1.0).sum()
    return klc

def logLikelihoodGaussian(x, mu, logvar):
    '''
    logLikelihoodGaussian(x, mu, logvar):
    
    Log-likeihood of a real-valued observation given a Gaussian distribution with mean 'mu' 
    and standard deviation 'exp(0.5*logvar)'
    
    Arguments:
        - x (torch.Tensor): Tensor of size batch-size x time-step x input dimensions
        - mu (torch.Tensor): Tensor of size batch-size x time-step x input dimensions
        - logvar (torch.tensor or torch.Tensor): tensor scalar or Tensor of size batch-size x time-step x input dimensions
    '''
    return -0.5*(log(2*pi) + logvar + ((x - mu).pow(2)/torch.exp(logvar))).sum()

def logLikelihoodPoisson(k, lam):
    '''
    logLikelihoodPoisson(k, lam)

    Log-likelihood of Poisson distributed counts k given intensity lam.

    Arguments:
        - k (torch.Tensor): Tensor of size batch-size x time-step x input dimensions
        - lam (torch.Tensor): Tensor of size batch-size x time-step x input dimensions
    '''
    return (k * torch.log(lam) - lam - torch.lgamma(k + 1)).sum()

def logLikelihoodEdgeworth(x, moments, ew_weight=1.0, clip_moments=False):
    '''
    logLikelihoodEdgeworth(x, mu, logvar, skew, logkurt):
    
    Log-likelihood of a real-valued observation modelled by a 4th-order Edgeworth series, i.e, a normal
    distribution multiplied by a polynomial. Used as a central moment expansion for the probability
    distribution of values generated by an exponential-kernel shot-noise/ filtered-poisson process.
    
    Arguments:
        - x (torch.Tensor): Tensor of size batch-size x time-step x input dimensions
        - mu (torch.Tensor): mean - 1st-moment. Tensor of size batch-size x time-step x input dimensions. 1-st order
        - logvar (torch.Tensor): log-variance - 2nd moment. Tensor of size batch-size x time-step x input dimensions
        - skew (torch.Tensor): skew. Tensor of size batch-size x time-step x input dimensions
        - logkurt (torch.Tensor): log-kurtosis. Tensor of size batch-size x time-step x input dimensions
        - ew_weight (float): weight of higher order Edgeworth terms on likelihood function (default=1.0)
    '''
    
    mu = moments[:, 0]
    logvar = moments[:, 1]
    
    clip_val = 0.8 if clip_moments else float('inf')
    ew_loss = 0
        
    if moments.shape[1] > 2:
        norm_skew = moments[:, 2] / torch.exp(1.5 * logvar)
        ew_loss += norm_skew.pow(2).sum()/norm_skew.numel()
        z = (x - mu)/torch.exp(0.5 * logvar)

        # Hermite Polynomials
        h3 = (z.pow(3) - 3 * z) / 6                           # 3rd-order Hermite
        ew3 = h3 * norm_skew.clamp(-clip_val, clip_val)       # 3rd-order Edgeworth        
        ew4 = 0
        
        if moments.shape[1] > 3:
            norm_kurt = moments[:, 3].exp() / torch.exp(2 * logvar)
            ew_loss += norm_kurt.pow(2).sum()/norm_kurt.numel()
            h4 = (z.pow(4) - 6 * z.pow(2) + 3) / 24                       # 4th-order Hermite 
            h6 = (z.pow(6) - 15 * z.pow(4) + 45 * z.pow(2) - 15) / 72     # 6th-order Hermite
            ew4 = h4 * norm_kurt.clamp(-clip_val, clip_val) + h6 * norm_skew.pow(2).clamp(-clip_val**2, clip_val**2)  # 4th-order Edgeworth

        # Gaussian term
        gauss = torch.exp(-z.pow(2)/2) * (2*pi*torch.exp(logvar)).pow(-0.5)

        # Edgeworth terms weighted to slowly introduce skew and kurtosis to distribution
        # Minimum probability set at 1e-8 as Edgeworth series can produce negative values
        return torch.log(torch.clamp((1 + ew_weight*(ew3 + ew4))*gauss, min=1e-8)).sum()
    else:
        return logLikelihoodGaussian(x, mu, logvar)

class LFADS_GRUCell(nn.Module):
    
    '''
    LFADS_GRUCell class. Implements the Gated Recurrent Unit (GRU) used in LFADS Encoders. More obvious
    relation to the equations (see https://en.wikipedia.org/wiki/Gated_recurrent_unit), along with
    a hack to help learning
    
    __init__(self, input_size, hidden_size, forget_bias=1.0)
    
    required arguments:
     - input_size (int) : size of inputs
     - hidden_size (int) : size of hidden state
     
    optional arguments:
     - forget_bias (float) : hack to help learning, added to update gate in sigmoid
    '''
    
    def __init__(self, input_size, hidden_size, forget_bias=1.0):
        super(LFADS_GRUCell, self).__init__()
        self.input_size  = input_size
        self.hidden_size = hidden_size
        self.forget_bias = forget_bias
        
        # Concatenated sizes
        self._xh_size = input_size + hidden_size
        self._ru_size = hidden_size * 2
        
        # r, u = W([x, h]) + b
        self.fc_xh_ru = nn.Linear(in_features= self._xh_size, out_features= self._ru_size)
        # c = W([x, h*r]) + b
        self.fc_xhr_c = nn.Linear(in_features= self._xh_size, out_features= self.hidden_size)
        
    def forward(self, x, h):
        '''
        Forward method - Gated Recurrent Unit forward pass with forget bias
        
        forward(self, x, h):
        
        required arguments:
          - x (torch.Tensor) : GRU input
          - h (torch.Tensor) : GRU hidden state
        
        returns
          - h_new (torch.Tensor) : updated GRU hidden state
        '''
        
        # Concatenate input and hidden state
        xh  = torch.cat([x, h], dim=1)
        
        # Compute reset gate and update gate vector
        r,u = torch.split(self.fc_xh_ru(xh),
                          split_size_or_sections=self.hidden_size,
                          dim = 1)
        r,u = torch.sigmoid(r), torch.sigmoid(u + self.forget_bias)
        
        # Concatenate input and hadamard product of hidden state and reset gate
        xrh = torch.cat([x, torch.mul(r, h)], dim=1)
        
        # Compute candidate hidden state
        c   = torch.tanh(self.fc_xhr_c(xrh))
        
        # Return new hidden state as a function of update gate, current hidden state, and candidate hidden state
        return torch.mul(u, h) + torch.mul(1 - u, c)
    
class LFADS_GenGRUCell(nn.Module):
    '''
    LFADS_GenGRUCell class. Implements gated recurrent unit used in LFADS generator and controller. Same as
    LFADS_GRUCell, but parameters transforming hidden state are kept separate for computing L2 cost (see 
    bullet point 2 of section 1.9 in online methods). Also does not create parameters transforming inputs if 
    no inputs exist.
    
    __init__(self, input_size, hidden_size, forget_bias=1.0)
    '''
    
    def __init__(self, input_size, hidden_size, forget_bias=1.0):
        super(LFADS_GenGRUCell, self).__init__()
        self.input_size  = input_size
        self.hidden_size = hidden_size
        self.forget_bias = forget_bias
        
        # Concatenated size
        self._ru_size    = self.hidden_size * 2
        
        # Create parameters for transforming inputs if inputs exist
        if self.input_size > 0:
            
            # rx ,ux = W(x) (No bias in tensorflow implementation)
            self.fc_x_ru = nn.Linear(in_features= self.input_size, out_features= self._ru_size, bias=False)
            # cx = W(x) (No bias in tensorflow implementation)
            self.fc_x_c  = nn.Linear(in_features= self.input_size, out_features= self.hidden_size, bias=False)
        
        # Create parameters transforming hidden state
        
        # rh, uh = W(h) + b
        self.fc_h_ru = nn.Linear(in_features= self.hidden_size, out_features= self._ru_size)
        # ch = W(h) + b
        self.fc_rh_c = nn.Linear(in_features=self.hidden_size, out_features= self.hidden_size)
        
    def forward(self, x, h):
        '''
        Forward method - Gated Recurrent Unit forward pass with forget bias, weight on inputs and hidden state kept separate.
        
        forward(self, x, h):
        
        required arguments:
          - x (torch.Tensor) : GRU input
          - h (torch.Tensor) : GRU hidden state
        
        returns
          - h_new (torch.Tensor) : updated GRU hidden state
        '''
        
        # Calculate reset and update gates from input
        if self.input_size > 0 and x is not None:
            r_x, u_x = torch.split(self.fc_x_ru(x),
                                   split_size_or_sections=self.hidden_size,
                                   dim = 1)
        else:
            r_x = 0
            u_x = 0
        
        # Calculate reset and update gates from hidden state
        r_h, u_h = torch.split(self.fc_h_ru(h),
                               split_size_or_sections=self.hidden_size,
                               dim = 1)
        
        # Combine reset and updates gates from hidden state and input
        r = torch.sigmoid(r_x + r_h)
        u = torch.sigmoid(u_x + u_h + self.forget_bias)
        
        # Calculate candidate hidden state from input
        if self.input_size > 0 and x is not None:
            c_x = self.fc_x_c(x)
        else:
            c_x = 0
        
        # Calculate candidate hidden state from hadamard product of hidden state and reset gate
        c_rh = self.fc_rh_c(h)
        
        # Combine candidate hidden state vectors
        c = torch.tanh(c_x + c_rh)
        
        # Return new hidden state as a function of update gate, current hidden state, and candidate hidden state
        return torch.mul(u, h) + torch.mul(1 - u, c)
    
    def hidden_weight_l2_norm(self):
        return self.fc_h_ru.weight.norm(2).pow(2)/self.fc_h_ru.weight.numel() + self.fc_rh_c.weight.norm(2).pow(2)/self.fc_rh_c.weight.numel()
    

#--------
# 1-D CAUSAL CHANNEL-SPECIFIC CONVOLUTION
#--------

class CausalChannelConv1d(torch.nn.Conv2d):
    def __init__(self,
                 kernel_size, out_channels=1, bias=True, infer_padding=False):
        
        # Setup padding
        if infer_padding:
            self.__padding = 0
        else:
            self.__padding = (kernel_size - 1)
        
        '''
        CausalChannelConv1d class. Implements channel-specific 1-D causal convolution. Applies same
        convolution kernel to every channel independently. Padding only at start of time dimension.
        Output dimensions are same as input dimensions.
        
        __init__(self, kernel_size, bias=True)
        
        required arguments:
            - kernel_size (int) : size of convolution kernel
            - out_channels (int) : number of output channels
            
        optional arguments:
            - bias (bool) : include bias (default=True)
        '''
        super(CausalChannelConv1d, self).__init__(
              in_channels=1,
              out_channels=out_channels,
              kernel_size = (kernel_size, 1),
              stride=1,
              padding=(self.__padding, 0),
              dilation=1,
              groups=1,
              bias=bias)
        
    def forward(self, input):
        # Include false channel dimension
        result = super(CausalChannelConv1d, self).forward(input.unsqueeze(1))
        if self.__padding != 0:
            # Slice tensor to include padding only at start and remove false channel dimension
            return result[:, :, :-self.__padding].squeeze(1)
        else:
            # Remove false channel dimension
            return result.squeeze(1)
    
#-------------------
# CORE LFADS NETWORK
#-------------------

class LFADS(nn.Module):
    
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def __init__(self, inputs_dim, T, dt,
                 model_hyperparams=None,
                 device = 'cpu', save_variables=False,
                 seed=None):
        '''
        LFADS_Net (Latent Factor Analysis via Dynamical Systems) neural network class.
        
        __init__(self, inputs_dim, T, dt,
                 model_hyperparams=None,
                 device='cpu', save_variables=False)
                 
            required arguments:
            
            - inputs_dim (int): the dimensionality of the data (e.g. number of cells)
            - T (int): number of time-steps in one sequence (i.e. one data point)
            - dt (float): time-step in seconds
            
            optional arguments:
            - device (string): device to use (default= 'cpu')
            - save_variables (bool) : whether to save dynamic variables (default= False)
            - model_hyperparams (dict) : dictionary of model_hyperparameters
                - ### DATA HYPERPARAMETERS ### 
                - datatype (string) : datatype (default = 'calcium')
                - dataset_name (String): name given to identify dataset (default = 'unknown')
                - run_name (String): name given to identify model run (default = 'tmp')
                
                - ### MODEL DIMENSIONS ###
                - g_dim (int): dimensionality of the generator (default = 100)
                - u_dim (int): dimensionality of the inferred inputs to the generator (default = 1)
                - factors_dim (int): dimensionality of the latent factors (default = 20)
                - g0_encoder_dim (int): dimensionality of the encoder for the initial conditions for the generator
                                        (default = 100)
                - c_encoder_dim (int): dimensionality of the encoder for the controller (default = 100)
                - controller_dim (int): dimensionality of controller (default = 100)
                
                - ### LATENT VARIABLE HYPERPARAMETERS ###
                - g0_prior_kappa (float) : initial variance for the learnable prior over the initial 
                                             generator state (default = 0.1)
                - g0_prior_var_min (float) : minimum variance for learnable prior over initial generator state.
                                             If none, set to g0_prior_kappa (default=None)
                - g0_prior_var_max (float) : maximum variance for learnable prior over initial generator state.
                                             If none, set to g0_prior_kappa (default=None)
                - g0_post_var_min (float) : minimum variance for posterior distribution over initial generator state.
                                            (default=0.0001)
                - g0_post_var_max (float) : maximum variance for posterior distribution over initial generator state.
                                            (default=np.inf)
                - u_prior_kappa (float): initial variance for the leanable prior over the inferred inputs
                                            to generator (default = 0.1)
                - u_prior_var_min (float) : minimum variance for learnable prior over inferred inputs.
                                             If none, set to u_prior_kappa (default=None)
                - u_prior_var_max (float) : maximum variance for learnable prior over inferred inputs.
                                             If none, set to u_prior_kappa (default=None)
                - u_prior_tau (float) : initial auto-regressive parameter for inferred inputs to generator (default=10)
                - u_prior_tau_min (float) : minimum  AR decay parameter for inferred inputs to generator (default = 0.1)
                - u_prior_tau_max (float) : maximum AR decay parameter for inferred inputs to generator (default = np.inf)
                - u_post_var_min (float) : minimum variance for posterior distribution over inferred inputs
                                           (default=0.0001)
                - u_post_var_max (float) : maximum variance for posterior distribution over inferred inputs 
                                           (default=np.inf)
                                                           
                - ### MISC HYPERPARAMETERS ###
                - keep_prob (float): keep probability for drop-out layers, if < 1 (default = 1.0)
                - clip_val (float): clips the hidden unit activity to be less than this value (default = 5.0)
                
                - ### OPTIMIZER HYPERPARAMETERS ###
                - lr (float): learning rate for ADAM optimizer (default = 0.01)
                - eps (float): epsilon value for ADAM optimizer (default = 0.1)
                - betas (2-tuple of floats): beta values for ADAM optimizer (default = (0.9, 0.999))
                - lr_decay (float): learning rate decay factor (default = 0.95)
                - lr_min (float): minimum learning rate (default = 1e-5)
                - max_norm (float): maximum gradient norm (default=200.0)
                - scheduler_on (bool): apply scheduler if True (default = True)
                - scheduler_patience (int): number of steps without loss decrease before weight decay (default = 6)
                - scheduler_cooldown (int): number of steps after weight decay to wait before next weight decay (default = 6)
                - kl_weight_min (float) : minimum weight on KL cost (default = 0.0)
                - kl_weight_schedule_start (int) : optimisation step to start kl_weight increase (default = 0)
                - kl_weight_schedule_dur (int) : number of optimisation steps to increase kl_weight to 1.0 (default = 2000)
                - l2_weight_min (float) : minimum weight on L2 cost (default = 0.0)
                - l2_weight_schedule_start (int) : optimisation step to start l2_weight increase (default = 0)
                - l2_weight_schedule_dur (int) : number of optimisation steps to increase l2_weight to 1.0 (default = 2000)
                - l2_gen_scale (float) : scaling factor for regularising l2 norm of generator hidden weights  (default = 0.0)
                - l2_con_scale (float) : scaling factor for regularising l2 norm of controller hidden weights (default = 0.0)
        '''
        
        # call the nn.Modules constructor
        super(LFADS, self).__init__()
        
        self.inputs_dim        = inputs_dim
        self.T                 = T
        self.dt                = dt

        self.device            = device
        self.save_variables    = save_variables
        self.seed              = seed
        
        self.model_hyperparams = model_hyperparams
        
        self._set_hyperparameters()
        
        self._initialize_encoder_net()
        self._initialize_generator_net()
        self._initialize_weights()
        self._initialize_priors()
        self._initialize_dropout()
        self._initialize_optimizer()
        
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    def _initialize_dropout(self):
        '''
        _initialize_dropout(self)
        
        Initialize dropout layer. Only needs to be done once and is applied to all uses
        '''
        
        self.dropout = nn.Dropout(1.0 - self.keep_prob)
        
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def _get_default_hyperparameters(self):
        
        '''
        _get_default_hyperparameters():
        
        Retrieve default hyperparameters for network
        
        Returns:
            - default_hyperparams (dict) : dictionary of default hyperparameters
        '''
        
        # Default hyperparameters
        default_hyperparams  = {### DATA PARAMETERS ###
                                'datatype'                 : 'spikes',
                                'dataset_name'             : 'unknown',
                                'run_name'                 : 'tmp',
                                
                                ### MODEL PARAMETERS ### 
                                'g0_dim'                   : 100,
                                'g_dim'                    : 100,
                                'u_dim'                    : 1,
                                'factors_dim'              : 20,
                                'g0_encoder_dim'           : 100,
                                'c_encoder_dim'            : 100,
                                'c_controller_dim'         : 100,
                                'g0_prior_kappa'           : 0.1,
                                'g0_prior_var_min'         : None,
                                'g0_prior_var_max'         : None,
                                'g0_post_var_min'          : 0.0001,
                                'g0_post_var_max'          : float('inf'),
                                'u_prior_kappa'            : 0.1,
                                'u_prior_tau'              : 10,
                                'u_prior_tau_min'          : 0.1,
                                'u_prior_tau_max'          : float('inf'),
                                'u_prior_var_min'          : None,
                                'u_prior_var_max'          : None,
                                'u_post_var_min'           : 0.0001,
                                'u_post_var_max'           : float('inf'), 
                                'keep_prob'                : 1.0,
                                'clip_val'                 : 5.0,
                                'norm_factors'             : True,
            
                                ### OPTIMIZER PARAMETERS ###
                                'learning_rate'            : 0.01,
                                'learning_rate_min'        : 1e-5,
                                'learning_rate_decay'      : 0.95,
                                'max_norm'                 : 200,
                                'scheduler_on'             : True,
                                'scheduler_patience'       : 6,
                                'scheduler_cooldown'       : 6,
                                'epsilon'                  : 0.1,
                                'betas'                    : (0.9, 0.99),
                                'l2_gen_scale'             : 0.0,
                                'l2_con_scale'             : 0.0,
                                'use_weight_schedule_fn'   : True,
                                'kl_weight_min'            : 0.0,
                                'kl_weight_schedule_start' : 0,
                                'kl_weight_schedule_dur'   : 2000,
                                'l2_weight_min'            : 0.0,
                                'l2_weight_schedule_start' : 0,
                                'l2_weight_schedule_dur'   : 2000}
        
        return default_hyperparams
        
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    def _set_hyperparameters(self):
        '''
        _set_hyperparameters():
        
        Set network hyperparameters
        '''
        
        # Check if model already has hyperparameters set as attributes
        if self.model_hyperparams is not None:
            for param in self.model_hyperparams.keys():
                if hasattr(self, param):
                    self.model_hyperparams[param] = getattr(self, param)
        
        # Get default hyperparameters
        default_hyperparams  = self._get_default_hyperparameters()
        
        # Store the hyperparameters        
        self._update_params(default_hyperparams, self.model_hyperparams)
        
        # Generate seed if not seed given
        if self.seed is None:
            self.seed = random.randint(1, 10000)
            print('Random seed: {}'.format(self.seed))
        else:
            print('Preset seed: {}'.format(self.seed))
        
        # Set seed across all modules and devices
        random.seed(self.seed)
        np.random.seed(self.seed)
        torch.manual_seed(self.seed)
        if self.device == 'cuda':
            torch.cuda.manual_seed_all(self.seed)
            
        # Store loss
        self.full_loss_store = {'train_loss' : {}, 'train_recon_loss' : {}, 'train_kl_loss' : {},
                                'valid_loss' : {}, 'valid_recon_loss' : {}, 'valid_kl_loss' : {},
                                'l2_loss' : {}}
        
        self.train_loss_store = []
        self.valid_loss_store = []
        self.best = np.inf
        
        # Training variables
        self.epochs = 0
        self.current_step = 0
        self.last_decay_epoch = 0
        
        # Cost function weight dictionary
        self.cost_weights = {'kl' : {'weight': self.kl_weight_min,
                                     'weight_min' : self.kl_weight_min,
                                     'schedule_start': self.kl_weight_schedule_start,
                                     'schedule_dur': self.kl_weight_schedule_dur},
                             'l2' : {'weight': self.l2_weight_min,
                                     'weight_min' : self.l2_weight_min,
                                     'schedule_start': self.l2_weight_schedule_start,
                                     'schedule_dur': self.l2_weight_schedule_dur}}
        
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    def _initialize_input_layers(self):
        '''
        _initialize_input_layers()
        
        Initialize layers that receive data as input
        '''

        # Notation:
        #
        #   layertype_outputvariable(_direction)
        #
        #   Examples: fc_factors= "fully connected layer, variable = factors"
        #             gru_Egen_forward = "gated recurrent unit layer, encoder for generator, forward direction"

        
        # Bidirectional RNN - Encoder g0
        self.gru_Egen_g0  = nn.GRU(input_size=self.inputs_dim, hidden_size=self.g0_encoder_dim, bidirectional=True)
        
        if self.u_dim > 0:
            
            # Bidirectional RNN - Encoder c
            self.gru_Econ_c          = nn.GRU(input_size= self.inputs_dim, hidden_size= self.c_encoder_dim, bidirectional=True)
        
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    def _initialize_encoder_net(self, requires_grad=True):
        '''
        _initialize_encoder_net():
        
        Initialize modules for encoder network
        
        Arguments:
            - requires_grad (bool) : set requires_grad attribute of all encoder network parameters
        '''
        
        self._initialize_input_layers()
        
        # Notation:
        #
        #   layertype_outputvariable(_direction)
        #
        #   Examples: fc_factors= "fully connected layer, variable = factors"
        #             gru_Egen_forward = "gated recurrent unit layer, encoder for generator, forward direction"
            
        # Generator forward encoder initial hidden state
        self.efgen_g0_init = nn.Parameter(torch.zeros(self.g0_encoder_dim), requires_grad=requires_grad)
        
        # Generator backward encoder initial hidden state
        self.ebgen_g0_init = nn.Parameter(torch.zeros(self.g0_encoder_dim), requires_grad=requires_grad)
        
        # mean and logvar of the posterior distribution for the generator initial conditions (g0 from E_gen_g0)
        # takes as inputs:
        #  - the forward encoder for g0 at time T (g0_enc_f_T)
        #  - the backward encoder for g0 at time 1 (g0_enc_b_0]
        self.fc_g0mean   = nn.Linear(in_features= 2 * self.g0_encoder_dim, out_features= self.g0_dim)
        self.fc_g0logvar = nn.Linear(in_features= 2 * self.g0_encoder_dim, out_features= self.g0_dim)
        
        apply_requires_grad(module = self.fc_g0mean, requires_grad=requires_grad)
        apply_requires_grad(module = self.fc_g0logvar, requires_grad=requires_grad)

        
        if self.u_dim > 0:
            
            # Controller c
            self.gru_controller_c    = LFADS_GenGRUCell(input_size= self.c_encoder_dim * 2 + self.factors_dim, hidden_size= self.c_controller_dim)
            
            # Controller forward encoder initial hidden state
            self.efcon_c_init = nn.Parameter(torch.zeros(self.c_encoder_dim), requires_grad=requires_grad)

            # Controller backward encoder initial hidden state
            self.ebcon_c_init = nn.Parameter(torch.zeros(self.c_encoder_dim), requires_grad=requires_grad)
            
            # External input Controller initial hidden state
            self.c_init       = nn.Parameter(torch.zeros(self.c_controller_dim), requires_grad=requires_grad)
            
            # mean and logvar of the approximate posterior distribution for the inferred inputs (u provided to g)
            # takes as inputs:
            #  - the controller at time t (c_t)
            self.fc_umean   = nn.Linear(in_features= self.c_controller_dim, out_features= self.u_dim)
            self.fc_ulogvar = nn.Linear(in_features= self.c_controller_dim, out_features= self.u_dim)
            
            apply_requires_grad(module = self.gru_controller_c, requires_grad=requires_grad)
            apply_requires_grad(module = self.fc_umean, requires_grad=requires_grad)
            apply_requires_grad(module = self.fc_ulogvar, requires_grad=requires_grad)
            
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    def _initialize_generator_net(self, requires_grad=True):
        '''
        _initialize_generator_net():
        
        Initialize modules for generator network
        '''
        
        # Notation:
        #
        #   layertype_outputvariable(_direction)
        #
        #   Examples: fc_factors= "fully connected layer, variable = factors"
        #             gru_Egen_forward = "gated recurrent unit layer, encoder for generator, forward direction"
        
        # g0 to generator initial condition
        if self.g0_dim == self.g_dim:
            self.fc_icgen = Identity(in_features=self.g0_dim, out_features=self.g_dim)
        else:
            self.fc_icgen         = nn.Linear(in_features=self.g0_dim, out_features= self.g_dim)
        
        # Generator RNN
        self.gru_generator    = LFADS_GenGRUCell(input_size= self.u_dim, hidden_size= self.g_dim)
        
        # -----------
        # Fully connected layers
        # -----------
                
        # factors from generator output
        self.fc_factors = nn.Linear(in_features= self.g_dim, out_features= self.factors_dim, bias=False)
        
        # poisson process rates from factors
        self.fc_logrates = nn.Linear(in_features= self.factors_dim, out_features= self.inputs_dim)
        
        apply_requires_grad(module = self.fc_icgen, requires_grad=requires_grad)
        apply_requires_grad(module = self.gru_generator, requires_grad=requires_grad)
        apply_requires_grad(module = self.fc_factors, requires_grad=requires_grad)
        apply_requires_grad(module = self.fc_logrates, requires_grad=requires_grad)
        
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    def _initialize_weights(self):
        '''
        _initialize_weights()
        
        Initialize network weights
        '''
    
        # TO-DO assign weight initialization method e.g., He, Xavier, etc.
    
        # -----------------------
        # WEIGHT INIT
        # 
        # The weight initialization is modified from the standard PyTorch, which is uniform. Instead,
        # the weights are drawn from a normal distribution with mean 0 and std = 1/sqrt(K) where K
        # is the size of the input dimension. This helps prevent vanishing/exploding gradients by
        # keeping the eigenvalues of the Jacobian close to 1.
        # -----------------------
        
        def standard_init(weights):
            k = weights.shape[1] # dimensionality of inputs
            weights.data.normal_(std=k**-0.5) # inplace resetting of W ~ N(0, 1/sqrt(N))
            
        
        # Step through all layers and adjust the weight initiazition method accordingly
        for m in self.modules():
            
            # GRU layer, update using input weight and recurrent weight dimensionality
            if isinstance(m, nn.GRUCell):
                standard_init(m.weight_ih)
                standard_init(m.weight_hh)
                
            if isinstance(m, nn.GRU):
                if m.bidirectional:
                    standard_init(m.weight_ih_l0_reverse)
                    standard_init(m.weight_hh_l0_reverse)
                    
                standard_init(m.weight_ih_l0)
                standard_init(m.weight_hh_l0)
            
            if isinstance(m, LFADS_GRUCell) or isinstance(m, LFADS_GenGRUCell):
                for g_m in m.modules():
                    if isinstance(g_m, nn.Linear):
                        standard_init(g_m.weight)                        
            
            # FC layer, update using input dimensionality
            elif isinstance(m, nn.Linear):
                standard_init(m.weight)
                
        if self.norm_factors:
            # Row-normalise fc_factors (See bullet-point 11 of section 1.9 of online methods)
            self.fc_factors.weight.data = F.normalize(self.fc_factors.weight.data, dim=1)
        
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    def _initialize_priors(self, requires_grad=True):
        '''
        _initialize_priors()
        
        Initialize parameters of prior distribution
        '''
    
        # -------------------------------
        # LEARNABLE PRIOR PARAMETERS INIT
        # -------------------------------
    
        # Set prior mean for generator initial conditions g0
        self.g0_prior_mu          = nn.Parameter(torch.zeros(self.g0_dim), requires_grad=requires_grad)
        
        # Check if prior variance for initial conditions has a minimum and maximum hyperparameter,
        # if not, set to be same as inital prior variance
        if self.g0_prior_var_min is None:
            self.g0_prior_var_min = self.g0_prior_kappa
        if self.g0_prior_var_max is None:
            self.g0_prior_var_max = self.g0_prior_kappa
        
        # if prior variance minimum and maximum are the same, set to be fixed throughout training
        if np.isclose(self.g0_prior_var_min, self.g0_prior_var_max):
            self.g0_prior_logkappa = torch.ones(self.g0_dim).to(self.device) * log(self.g0_prior_kappa)
            
        # if prior variance minimum and maximum are different, set to be a trainable parameter
        else:
            self.g0_prior_logkappa = nn.Parameter(torch.ones(self.g0_dim)  * log(self.g0_prior_kappa), requires_grad=requires_grad)
                
        # Setup generator input prior parameters (if necessary)
        if self.u_dim > 0:
            # Set prior mean for inputs
            self.u_prior_mu       = torch.zeros(self.u_dim, device=self.device)
            
            # Set prior autoregressive term for inputs 
            self.u_prior_logtau      = nn.Parameter(torch.ones(self.u_dim) * log(self.u_prior_tau), requires_grad=requires_grad)
            
            # Check if prior variance for generator inputs has a minimum and maximum hyperparameter,
            # if not, set to be same as inital prior variance
            if self.u_prior_var_min is None:
                self.u_prior_var_min = self.u_prior_kappa
            if self.u_prior_var_max is None:
                self.u_prior_var_max = self.u_prior_kappa
                
            # if prior variance minimum and maximum are the same, set to be fixed throughout training
            if np.isclose(self.u_prior_var_min, self.u_prior_var_max):
                self.u_prior_logkappa = torch.ones(self.u_dim).to(self.device) * log(self.u_prior_kappa)
            
            # if prior variance minimum and maximum are different, set to be a trainable parameter
            else:
                self.u_prior_logkappa = nn.Parameter(torch.ones(self.u_dim) * log(self.u_prior_kappa), requires_grad=requires_grad)

    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def _initialize_optimizer(self):
        '''
        _initialize_optimizer()
        
        Initialize optimizer and likelihood function
        '''
        
        # --------------------------
        # LOG-LIKELIHOOD FUNCTION
        # --------------------------
        
        self.logLikelihood = logLikelihoodPoisson

        # --------------------------
        # OPTIMIZER INIT WITHOUT FROZEN WEIGHTS
        # --------------------------
        self.optimizer = opt.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=self.learning_rate, eps=self.epsilon, betas=self.betas)
        
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
        
    def initialize_variables(self, batch_size=None):
        '''
        initialize_variables(batch_size=None)
        
        Initialize dynamic model variables. These need to be reinitialized with each forward pass to
        ensure we don't need to retain graph between each .backward() call. 
        
        See https://discuss.pytorch.org/t/what-exactly-does-retain-variables-true-in-loss-backward-do/3508/2
        for discussion and explanation
        
        Note: The T + 1 terms  accommodate learnable biases for all variables, except for the generator,
        which is provided with a g0 estimate from the network
        
        optional arguments:
          batch_size (int) : batch dimension. If None, use self.batch_size.
        
        '''
        
        # Set batch_size
        batch_size = batch_size if batch_size is not None else self.batch_size
               
        # Initialise initial condition prior mean with new batch dimension
        self.g0_prior_mean   = torch.ones(batch_size, self.g0_dim).to(self.device) * self.g0_prior_mu          # g0 prior mean
        
        # Initialise initial condition prior logvariance with new batch dimension
        self.g0_prior_logvar = torch.ones(batch_size, self.g0_dim).to(self.device) * torch.clamp(self.g0_prior_logkappa, log(self.g0_prior_var_min), log(self.g0_prior_var_max))    # g0 prior logvar
                        
        # Initialise generator encoder inital state with new batch dimension
        self.efgen_g0 = torch.ones((batch_size, self.g0_encoder_dim)).to(self.device) * self.efgen_g0_init  # Forward generator encoder
        self.ebgen_g0 = torch.ones((batch_size, self.g0_encoder_dim)).to(self.device) * self.ebgen_g0_init  # Backward generator encoder
        
        if self.u_dim > 0:
            # Initialise external input prior mean with new batch dimension
            self.u_prior_mean    = torch.ones(batch_size, self.u_dim).to(self.device) * self.u_prior_mu           # u prior mean
            
            # Initialise initial condition prior logvariance with new batch dimension
            self.u_prior_logvar  = torch.ones(batch_size, self.u_dim).to(self.device) * torch.clamp(self.u_prior_logkappa, log(self.u_prior_var_min), log(self.u_prior_var_max))     # u prior logvar
            
            # Initialise initial condition prior autocorrelation with new batch dimension
            self.u_prior_tau   = torch.ones(batch_size, self.u_dim).to(self.device) * self.u_prior_logtau.exp()          # u prior_tau 
            
            self.efcon_c = torch.ones((batch_size, self.c_encoder_dim)).to(self.device) * self.efcon_c_init  # Forward controller encoder
            self.ebcon_c = torch.ones((batch_size, self.c_encoder_dim)).to(self.device) * self.ebcon_c_init  # Backward controller encoder
            
            # Initialise controller state
            self.c = torch.ones((batch_size, self.c_controller_dim)).to(self.device) * self.c_init # Controller hidden state
            
        # Initialise state variables for storage
        if self.save_variables:
            self.factors       = torch.zeros(batch_size, self.T, self.factors_dim)
            self.rates         = torch.zeros(batch_size, self.T, self.inputs_dim)
            if self.u_dim > 0:
                self.inputs        = torch.zeros(batch_size, self.T, self.u_dim)
                self.inputs_mean   = torch.zeros(batch_size, self.T, self.u_dim)
                self.inputs_logvar = torch.zeros(batch_size, self.T, self.u_dim)
                
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
                
    def encode(self, x):
        '''
        encode(x)
        
        Function to encode the data with the forward and backward encoders.
        
        Arguments:
          - x (torch.Tensor): Single-trial data of size batch size x time-steps x input dimension
        '''
        
        # Dropout some data
        if self.keep_prob < 1.0:
            x = self.dropout(x)
        
        # Pass bidirectional encoder RNN for g0 (generator initial state) over data
        out_gru_egen_g0, hidden_gru_egen_g0 = self.gru_Egen_g0(x, torch.stack((self.efgen_g0, self.ebgen_g0)))
        # Dropout final hidden state of bi-RNN and clip values
        self.egen_g0_1T_dropout  = self.dropout(hidden_gru_egen_g0.clamp(min=-self.clip_val, max=self.clip_val))
        # Concatenate final hidden state of bi-RNN
        self.egen_g0_1T_dropout  = torch.cat((self.egen_g0_1T_dropout[0], self.egen_g0_1T_dropout[1]), dim=1)
        
        if self.u_dim > 0:
            # Pass bidirectional encoder RNN for controller input (econ_c_t) over data
            out_gru_econ_c, hidden_gru_econ_c = self.gru_Econ_c(x, torch.stack((self.efcon_c, self.ebcon_c)))
            
            # Get controller input for each time step as output of bidirectional RNN (econ_c_1tT) and clip values
            self.econ_c_1tT = out_gru_econ_c.clamp(min=-self.clip_val, max=self.clip_val)
        
        # Compute parameters of g0 distribution
        self.g0_mean   = self.fc_g0mean(self.egen_g0_1T_dropout)
        self.g0_logvar = self.fc_g0logvar(self.egen_g0_1T_dropout)
        
        # Sample initial generator state
        self.g0         = Variable(torch.randn(self.batch_size, self.g0_dim).to(self.device))*torch.exp(0.5 * self.g0_logvar)\
                         + self.g0_mean
        
        # Dropout some of the generator state for sending to factors, but keep generator state intact for feeding back to generator
        self.g      = self.fc_icgen(self.g0)
        
        if self.keep_prob < 1.0:
            self.g_do   = self.dropout(self.g)
        
        # Initialise factors
        self.f         = self.fc_factors(self.g_do)
        
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def compute_recon_loss_one_step(self, x, t):
        '''
        compute_recon_loss_one_step(x, t)
        
        Compute reconstruction loss for one time step and add to recon_loss of model
        
        Arguments:
            - x (torch.Tensor) : Single trial data
            - t (int) : time-step
        '''
        if t == -1:
            self.recon_loss = 0
        else:
            self.recon_loss = self.recon_loss - self.logLikelihood(x[t], self.r * self.dt)/self.batch_size
        
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def compute_kl_loss_one_step(self, t):
        '''
        compute_kl_loss_one_step(t):
        
        Compute kl loss for one time step and add to kl_loss of model
        
        Arguments:
            - t (int) : time-step
        '''
        if t == -1:
            # KL cost for g(0)
            self.kl_loss =  KLCostGaussian(self.g0_mean, self.g0_logvar,
                                           self.g0_prior_mean, self.g0_prior_logvar)/self.batch_size
        
        else:
            if self.u_dim > 0:
            # KL cost for u(t)
                if t == 0:
                    self.kl_loss = self.kl_loss + KLCostGaussian(self.u_mean, self.u_logvar,
                                                   self.u_prior_mean, self.u_prior_logvar)/self.batch_size


                elif t > 0:
                    # Calculate autoregressive KL loss
                    gp_prior_mean   = (self.u_prev - self.u_prior_mean) * torch.exp(-1/self.u_prior_tau) + self.u_prior_mean
                    gp_prior_logvar = torch.log(1 - torch.exp(-1/self.u_prior_tau).pow(2)) + self.u_prior_logvar

                    self.kl_loss = self.kl_loss + KLCostGaussian(self.u_mean, self.u_logvar,
                                                   gp_prior_mean, gp_prior_logvar)/self.batch_size
                self.u_prev = self.u.clone()

            
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def compute_loss_one_step(self, x, t):
        self.compute_recon_loss_one_step(x, t)
        self.compute_kl_loss_one_step(t)
    
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def generate_one_step(self, t):
        '''
        generate_one_step(t)
        
        Sample from approximate posterior, update generator and reconstruct input for one time step
        
        Arguments:
            - t (int) : time-step
        '''
        
        # Concatenate ebcon_c and efcon_c outputs at time t with factors at time t-1 as input to controller for external inputs
        # Note: we take efcon at t+1, because the learnable biases are at first index for efcon
        if self.u_dim > 0:
            econ_c_and_fac = torch.cat((self.econ_c_1tT[t], self.f), dim= 1)

            # Dropout the controller encoder outputs and factors
            if self.keep_prob < 1.0:
                econ_c_and_fac = self.dropout(econ_c_and_fac)

            # Update controller with controller encoder outputs
            self.c = torch.clamp(self.gru_controller_c(econ_c_and_fac, self.c), min= -self.clip_val, max=self.clip_val)

            # Calculate posterior distribution parameters for inferred inputs from controller state
            self.u_mean   = self.fc_umean(self.c)
            self.u_logvar = torch.clamp(self.fc_ulogvar(self.c), min=log(self.u_post_var_min), max = log(self.u_post_var_max))

            # Sample inputs for generator from u(t) posterior distribution
            self.u = Variable(torch.randn(self.batch_size, self.u_dim).to(self.device))*torch.exp(0.5*self.u_logvar) \
                        + self.u_mean
        else:
            self.u = None

        # Update generator
        self.g = torch.clamp(self.gru_generator(self.u, self.g), min=-self.clip_val, max=self.clip_val)

        # Dropout on generator output, but don't overwrite generator state sent back to generator at next time step
        if self.keep_prob < 1.0:
            self.g_do = self.dropout(self.g)

        # Generate factors from generator state
        self.f = self.fc_factors(self.g_do)

        # Generate rate from factors
        self.r = self.fc_logrates(self.f).exp()

        if self.save_variables and t >= 0:
            self.factors[:, t] = self.f.detach().cpu()
            self.rates[:, t] = self.r.detach().cpu()
            if self.u_dim > 0:
                self.inputs[:, t] = self.u.detach().cpu()
                self.inputs_mean[:, t] = self.u_mean.detach().cpu()
                self.inputs_logvar[:, t] = self.u_logvar.detach().cpu()
                
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def generate(self, x):
        '''
        generate()
        
        Generates the rates using the controller encoder outputs and the sampled initial conditions for
        generator.
        
        Arguments:
        - x (torch.Tensor): Single-trial data. Tensor of size batch_size x time_steps x input_size
        '''
        
        self.compute_loss_one_step(x, -1)
        for t in range(self.T):
            self.generate_one_step(t)
            self.compute_loss_one_step(x, t)
            
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    def forward(self, x):
        '''
        forward(x)
        
        Runs a forward pass through the network.
        
        Arguments:
          - x (torch.Tensor): Single-trial spike data. Tensor of size batch size x time-steps x input dimension
        '''
        batch_size, steps_dim, inputs_dim = x.shape
        
        assert steps_dim  == self.T
        assert inputs_dim == self.inputs_dim
        
        self.batch_size = batch_size
        self.initialize_variables(batch_size=batch_size)
        self.encode(x.permute(1, 0, 2))
        self.generate(x.permute(1, 0, 2))
        
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def forward_no_grad(self, x):
        '''
        forward_no_grad(x)
        
        Runs a forward pass through the network without tracking gradients.
        '''
        self.eval()
        self.batch_size = x.shape[0]
        prev_save = self.save_variables
        with torch.no_grad():
            self.save_variables = True
            self(x)
        self.save_variables = prev_save
    
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    def reconstruct(self, x):
        '''
        reconstruct(x)
        
        Runs a forward pass through the network, and outputs reconstruction of data x. History is not tracked.
        
        Arguments:
          - x (torch.Tensor): Single-trial data. Tensor of size batch size x time-steps x input dimensions
          
        Returns:
          - x_recon (torch.Tensor): Reconstruction. Tensor of size batch size x time-steps x input dimensions
        '''
        self.forward_no_grad(x)
        return self.rates.mean(dim=0).cpu().numpy()
    
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def infer_factors(self, x):
        '''
        infer_factors(x)
        
        Runs a forward pass through the network, and outputs latent factors. History is not tracked.
        
        Arguments:
          - x (torch.Tensor): Single-trial spike data. Tensor of size batch size x time-steps x input dimensions
          
        Returns:
          - f (torch.Tensor): Single-trial factors. Tensor of size batch size x time-steps x factor dimensions
        '''
        self.forward_no_grad(x)
        return self.factors.detach().cpu()
    
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------

    def weight_schedule_fn(self, step):
        '''
        weight_schedule_fn(step)
        
        Calculate the KL and L2 regularization weights from the current training step number. Imposes
        linearly increasing schedule on regularization weights to prevent early pathological minimization
        of KL divergence and L2 norm before sufficient data reconstruction improvement. See bullet-point
        4 of section 1.9 in online methods
        
        required arguments:
            - step (int) : training step number
        '''
        
        for cost_key in self.cost_weights.keys():
            # Get step number of scheduler
            weight_step = max(step - self.cost_weights[cost_key]['schedule_start'], 0)
            
            # Calculate schedule weight
            self.cost_weights[cost_key]['weight'] = max(min(weight_step/ self.cost_weights[cost_key]['schedule_dur'], 1.0), self.cost_weights[cost_key]['weight_min'])
    
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def apply_decay(self, current_loss):
        '''
        apply_decay(current_loss)
        
        Decrease the learning rate by a defined factor (self.learning_rate_decay) if loss is greater
        than the loss in the last six training steps and if the loss has not decreased in the last
        six training steps. See bullet point 8 of section 1.9 in online methods
        
        Arguments:
            - current_loss (float) : loss for current epoch
        '''
        
        if len(self.train_loss_store) >= self.scheduler_patience:
            if all((current_loss > past_loss for past_loss in self.train_loss_store[-self.scheduler_patience:])):
                if self.epochs >= self.last_decay_epoch + self.scheduler_cooldown:
                    self.learning_rate  = self.learning_rate * self.learning_rate_decay
                    self.last_decay_epoch = self.epochs
                    for g in self.optimizer.param_groups:
                        g['lr'] = self.learning_rate
                    print('Learning rate decreased to %.6f'%self.learning_rate)
                    
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def _make_save_loc(self):
        '''
        _make_save_loc()
        
        Create path for save location and model directory
        '''
    
        self.model_dir = '%s/models/%s_%s/%s/'%(self.home_dir, self.dataset_name, self.datatype, self.run_name)
        self.save_loc  = self.model_dir + 'checkpoints/'

    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def _make_save_loc_dir(self):
        '''
        _make_save_loc_dir()
        
        Make model directory, overwriting if it already exists
        '''
        
        # Create model_checkpoint directory if it doesn't exist
        if not os.path.isdir(self.save_loc):
            os.makedirs(self.save_loc)
        elif os.path.exists(self.save_loc) and self.epochs==0:
            os.system('rm -rf %s'%self.save_loc)
            os.makedirs(self.save_loc)
    
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def _prepare_fit(self, train_dataset, valid_dataset, train_truth=None, valid_truth=None, batch_size=4, use_tensorboard=False, home_dir='.'):
        '''
        _prepare_fit(train_dataset, valid_dataset, train_truth, valid_truth, batch_size, use_tensorboard, home_dir)
        
        Prepare dictionary with dataloaders and ground-truth if provided. If plotting with tensorboard, setup writer
        and directory for tensorboard
        
        Required Arguments:
            train_dataset (torch.Dataset) : training dataset
            valid_dataset (torch.Dataset) : validation dataset

        Optional Arguments:
            train_truth (dict) : dictionary of ground truth training data (default = None)
            valid_truth (dict) : dictionary of ground truth validation data (default = None)
            use_tensorboard (bool) : Whether to use tensorboard (default = False)
            home_dir (str) : home directory to reference saves (default = '.')
            
        Returns:
            -fit_dict (dict)
        '''
    
        # Set Training Loop parameters
        self.batch_size = batch_size
        self.home_dir = home_dir

        # create the dataloader
        train_dl = torch.utils.data.DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)
        valid_dl = torch.utils.data.DataLoader(valid_dataset, batch_size=self.batch_size)
        
        fit_dict = {'train_dl' : train_dl, 'valid_dl' : valid_dl,
                    'train_truth' : train_truth, 'valid_truth' : valid_truth}
        
        # Initialize directory to save checkpoints
        self._make_save_loc()
        self._make_save_loc_dir()
            
        # Initialize tensorboard
        fit_dict['use_tensorboard'] = use_tensorboard
        if use_tensorboard:
            tb_folder = self.model_dir + 'tensorboard/'
            if not os.path.exists(tb_folder):
                os.mkdir(tb_folder)
            elif os.path.exists(tb_folder) and self.epochs==0:
                os.system('rm -rf %s'%tb_folder)
                os.mkdir(tb_folder)
            
            from tensorboardX import SummaryWriter
            writer = SummaryWriter(tb_folder)
            fit_dict['writer'] = writer
            
        self.saved = 0
        # print a message
        print('Beginning training...')
        return fit_dict

    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def _compute_total_loss_one_step(self, x):
        
        '''
        _compute_loss_one_step(x)
        
        Zero gradients in optimizer, forward pass of model, then collect loss terms.
        
        Arguments:
            - x (torch.Tensor) : Data batch
        '''
        
        self.current_step += 1

        # apply Variable wrapper to batch
        x = Variable(x[0])

        # zero the parameter gradients
        self.optimizer.zero_grad()

        # Update regularizer weights
        if self.use_weight_schedule_fn:
            self.weight_schedule_fn(self.current_step)

        # Forward
        self(x)

        # Calculate l2 regularisation penalty
        self.l2_loss = 0.5 * self.l2_gen_scale * self.gru_generator.hidden_weight_l2_norm()
        if self.u_dim > 0:
            self.l2_loss += 0.5 * self.l2_con_scale * self.gru_controller_c.hidden_weight_l2_norm()

        # Collect separate weighted losses
        kl_weight = self.cost_weights['kl']['weight']
        l2_weight = self.cost_weights['l2']['weight']

        # Compute loss with KL and L2 warmup
        loss = self.recon_loss + kl_weight * self.kl_loss + l2_weight * self.l2_loss
        
        return loss
    
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def _normalize_weights(self):
        '''
        _normalize_weights()
        
        Weight adjustments after optimizer step
        '''
        # Row-normalise fc_factors (See bullet-point 11 of section 1.9 of online methods)
        if self.norm_factors:
            self.fc_factors.weight.data = F.normalize(self.fc_factors.weight.data, dim=1)
        
    
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def _train_one_epoch(self, fit_dict, health_check=False):
        '''
        _train_one_epoch(fit_dict, health_check)
        
        Train model for one epoch
        
        Required arguments: 
            - fit_dict (dict) : dictionary containing dataloaders
            - health_check (bool) : whether to run health check (default = False)
            
        Returns:
            - train_loss (float) : Total loss for training data in current epoch
            - train_recon_loss (float) : Reconstruction loss for training data in current epoch
            - train_kl_loss (float) : KL loss for training data in current epoch
        '''
        
        # Set model in training mode
        self.train()
        
        # Initialise running loss for epoch
        train_loss = 0
        train_recon_loss = 0
        train_kl_loss = 0

        # for each batch...
        for i, x in enumerate(fit_dict['train_dl'], 0):

            loss = self._compute_total_loss_one_step(x)

            # Check if loss is nan
            assert not torch.isnan(loss.data), 'Loss is NaN'

            # Backward
            loss.backward()

            # clip gradient norm
            torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=self.max_norm)

            # update the weights
            self.optimizer.step()
            
            self._normalize_weights()

            if health_check:
                if fit_dict['use_tensorboard']:
                    self.health_check(fit_dict['writer'])

            # Add batch loss to epoch running loss
            train_loss += loss.data
            train_recon_loss += self.recon_loss.data
            train_kl_loss += self.kl_loss.data

        train_loss /= (i+1)
        train_recon_loss /= (i+1)
        train_kl_loss /= (i+1)

        return train_loss, train_recon_loss, train_kl_loss
        
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def _test_one_epoch(self, fit_dict):
        '''
        _test_one_epoch(fit_dict)
        
        Test model parameters on validation dataset
        
        Arguments:
            - fit_dict (dict) : Dictionary containing dataloaders, ground truth and other information
            
        Returns:
            - test_loss (float)
            - test_recon_loss (float)
            - test_kl_loss (float)
        '''
        
        # Set model in evaluation mode
        self.eval()

        # Initialize running test loss for epoch
        test_loss = 0
        test_recon_loss = 0
        test_kl_loss = 0
        
        # For each batch...
        for i, x in enumerate(fit_dict['valid_dl'], 0):
            with torch.no_grad():
                # Wrap batch in Variable
                x = Variable(x[0])
                
                # Forward pass of model
                self(x)
                
                # Collect loss
                loss = self.recon_loss + self.kl_loss +self.l2_loss
                
                # Add losses to running loss
                test_loss += loss.data
                test_recon_loss += self.recon_loss.data
                test_kl_loss += self.kl_loss.data               
                
        # Average running loss to get epoch loss
        test_loss /= (i+1)
        test_recon_loss /= (i+1)
        test_kl_loss /= (i+1)
        return test_loss, test_recon_loss, test_kl_loss
    
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def _write_loss(self, writer):
        
        '''
        _write_loss(writer)
        
        Write loss to tensorboard
        
        Arguments:
            - writer (tensorboardX.Writer) : writer for tensorboard
        '''
        
        # Write loss to full_loss_store dict
        train_loss       = self.full_loss_store['train_loss'][self.epochs]
        train_recon_loss = self.full_loss_store['train_recon_loss'][self.epochs]
        train_kl_loss    = self.full_loss_store['train_kl_loss'][self.epochs]

        valid_loss       = self.full_loss_store['valid_loss'][self.epochs]
        valid_recon_loss = self.full_loss_store['valid_recon_loss'][self.epochs]
        valid_kl_loss    = self.full_loss_store['valid_kl_loss'][self.epochs]
        
        l2_loss = self.full_loss_store['l2_loss'][self.epochs]
        
        # Retrieve loss weights from cost_weights dict
        kl_weight = self.cost_weights['kl']['weight']
        l2_weight = self.cost_weights['l2']['weight']
        
        # Write loss to tensorboard
        writer.add_scalars('1_Loss/1_Total_Loss', {'Training' : float(train_loss), 
                                                 'Validation' : float(valid_loss)}, self.epochs)

        writer.add_scalars('1_Loss/2_Reconstruction_Loss', {'Training' :  float(train_recon_loss), 
                                                          'Validation' : float(valid_recon_loss)}, self.epochs)

        writer.add_scalars('1_Loss/3_KL_Loss' , {'Training' : float(train_kl_loss), 
                                               'Validation' : float(valid_kl_loss)}, self.epochs)

        writer.add_scalar('1_Loss/4_L2_loss', float(l2_loss), self.epochs)

        writer.add_scalar('2_Optimizer/1_Learning_Rate', self.learning_rate, self.epochs)
        writer.add_scalar('2_Optimizer/2_KL_weight', kl_weight, self.epochs)
        writer.add_scalar('2_Optimizer/3_L2_weight', l2_weight, self.epochs)
        
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
        
    def _visualise_checkpoint_tensorboard(self, figs_dict_train, figs_dict_valid, writer):
        
        '''
        _visualise_checkpoint_tensorboard(figs_dict_train, figs_dict_valid, writerS)
        
        Visualise checkpoint in tensorboard
        
        Arguments:
            - figs_dict_train (dict): dictionary containing figures of training data results
            - figs_dict_valid (dict): dictionary containing figures of validation data results
            - writer (tensorboardX.Writer): writer for tensorboard plotting
        '''
        
        writer.add_figure('Examples/1_Train', figs_dict_train['traces'], self.epochs, close=True)
        if self.u_dim > 0:
            writer.add_figure('Inputs/1_Train', figs_dict_train['inputs'], self.epochs, close=True)

        writer.add_figure('Examples/2_Valid', figs_dict_valid['traces'], self.epochs, close=True)
        if self.u_dim > 0:
            writer.add_figure('Inputs/2_Valid', figs_dict_valid['inputs'], self.epochs, close=True)

        if 'truth_factors' in figs_dict_train.keys():
            writer.add_figure('Ground_truth/1_Train/1_Factors', figs_dict_train['truth_factors'], self.epochs, close=True)
        else:
            writer.add_figure('Factors/1_Train', figs_dict_train['factors'], self.epochs, close=True)

        if 'truth_rates' in figs_dict_train.keys():
            writer.add_figure('Ground_truth/1_Train/2_Rates', figs_dict_train['truth_rates'], self.epochs, close=True)
        else:
            writer.add_figure('Rates/1_Train', figs_dict_train['rates'], self.epochs, close=True)
            
        if 'truth_factors' in figs_dict_valid.keys():
            writer.add_figure('Ground_truth/2_Valid/1_Factors', figs_dict_valid['truth_factors'], self.epochs, close=True)
        else:
            writer.add_figure('Factors/2_Valid', figs_dict_valid['factors'], self.epochs, close=True)

        if 'truth_rates' in figs_dict_valid.keys():
            writer.add_figure('Ground_truth/2_Valid/2_Rates', figs_dict_valid['truth_rates'], self.epochs, close=True)
        else:
            writer.add_figure('Rates/2_Valid', figs_dict_valid['rates'], self.epochs, close = True)
                    
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def save_checkpoint(self, force=False, purge_limit=50):
        '''
        Save checkpoint of network parameters and optimizer state
        
        Arguments:
            force (bool) : force checkpoint to be saved (default = False)
            purge_limit (int) : delete previous checkpoint if there have been fewer
                                epochs than this limit before saving again
        '''
        
        # output_filename of format [timestamp]_epoch_[epoch]_loss_[training].pth:
        #  - timestamp   (YYMMDDhhmm)
        #  - epoch       (int)
        #  - loss        (float with decimal point replaced by -)
        
        if force:
            pass
        else:
            if purge_limit:
                # Get checkpoint filenames
                try:
                    _,_,filenames = list(os.walk(self.save_loc))[0]
                    split_filenames = [os.path.splitext(fn)[0].split('_') for fn in filenames]
                    epochs = [att[2] for att in split_filenames]
                    epochs.sort()
                    last_saved_epoch = epochs[-1]
                    if self.epochs - 50 <= int(last_saved_epoch):
                        rm_filename = [filename for filename in filenames if last_saved_epoch in filename][0]
                        os.remove(self.save_loc+rm_filename)
                    
                except IndexError:
                    pass

        # Get current time in YYMMDDhhmm format
        timestamp = datetime.datetime.now().strftime('%y%m%d%H%M')
        
        # Get epoch_num as string
        epoch = str('%i'%self.epochs)
        
        # Get training_error as string
        loss = str(self.valid_loss_store[-1]).replace('.','-')
        
        output_filename = '%s_epoch_%s_loss_%s.pth'%(timestamp, epoch, loss)
        
        assert os.path.splitext(output_filename)[1] == '.pth', 'Output filename must have .pth extension'
                
        # Create dictionary of training variables
        train_dict = {'best' : self.best, 'train_loss_store': self.train_loss_store,
                      'valid_loss_store' : self.valid_loss_store,
                      'full_loss_store' : self.full_loss_store,
                      'epochs' : self.epochs, 'current_step' : self.current_step,
                      'last_decay_epoch' : self.last_decay_epoch,
                      'learning_rate' : self.learning_rate,
                      'cost_weights' : self.cost_weights,
                      'dataset_name' : self.dataset_name,
                      'datatype' : self.datatype}
        
        # Save network parameters, optimizer state, and training variables
        torch.save({'net' : self.state_dict(), 'opt' : self.optimizer.state_dict(), 'train' : train_dict},
                   self.save_loc+output_filename)

    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def load_checkpoint(self, input_filename='best', home_dir='.'):
        '''
        Load checkpoint of network parameters and optimizer state

        required arguments:
            - input_filename (string): options:
                - path to input file. Must have .pth extension
                - 'best' (default) : checkpoint with lowest saved loss
                - 'recent'         : most recent checkpoint
                - 'longest'        : checkpoint after most training
            
            - dataset_name : if input_filename is not a path, must not be None
        '''
        self.home_dir = home_dir
        self._make_save_loc()
        
        # If input_filename is not a filename, get checkpoint with specified quality (best, recent, longest)
        if not os.path.exists(input_filename):
            # Get checkpoint filenames
            input_filename = self._get_checkpoint_filename(input_filename)
            
        if input_filename:
            assert os.path.splitext(input_filename)[1] == '.pth', 'Input filename must have .pth extension'

            # Load checkpoint
            state = torch.load(self.save_loc+input_filename)

            # Set network parameters
            self.load_state_dict(state['net'])

            # Set training variables
            self.best                  = state['train']['best']
            self.train_loss_store      = state['train']['train_loss_store']
            self.valid_loss_store      = state['train']['valid_loss_store']
            self.full_loss_store       = state['train']['full_loss_store']
            self.epochs                = state['train']['epochs']
            self.current_step          = state['train']['current_step']
            self.last_decay_epoch      = state['train']['last_decay_epoch']
            self.learning_rate         = state['train']['learning_rate']
            self.cost_weights          = state['train']['cost_weights']
            self.dataset_name          = state['train']['dataset_name']
            self.datatype              = state['train']['datatype']

            self._load_optimizer_checkpoint(state['opt'])
        
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def _load_optimizer_checkpoint(self, opt_state):
        '''
        _load_optimizer_checkpoint(opt_state)
        
        load checkpoint for optimizer
        
        Arguments:
            - opt_state (dict) : optimizer state dict
        '''
        self.optimizer.load_state_dict(opt_state)
        
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def _get_checkpoint_filename(self, tag):
        '''
        _get_checkpoint_filename(tag):
        
        Get filename for checkpoint with tag 'best', 'recent', 'longest'
        
        Arguments:
            - tag (string) : tag to get checkpoint. Must be one of 'best', 'recent' or 'longest'
        
        '''
        
        # Get checkpoint filenames
        try:
            _,_,filenames = list(os.walk(self.save_loc))[0]
        except IndexError:
            return

        if len(filenames) > 0:

            # Sort in ascending order
            filenames.sort()

            # Split filenames into attributes (dates, epochs, loss)
            split_filenames = [os.path.splitext(fn)[0].split('_') for fn in filenames]
            dates = [att[0] for att in split_filenames]
            epoch = [att[2] for att in split_filenames]
            loss  = [att[-1] for att in split_filenames]

            if tag == 'best':
                # Get filename with lowest loss. If conflict, take most recent of subset.
                loss.sort()
                best = loss[0]
                input_filename = [fn for fn in filenames if best in fn][-1]

            elif tag == 'recent':
                # Get filename with most recent timestamp. If conflict, take first one
                dates.sort()
                recent = dates[-1]
                input_filename = [fn for fn in filenames if recent in fn][0]

            elif tag == 'longest':
                # Get filename with most number of epochs run. If conflict, take most recent of subset.
                epoch.sort()
                longest = epoch[-1]
                input_filename = [fn for fn in filenames if longest in fn][-1]

            else:
                assert False, 'tag must be one of \'best\', \'recent\', or \'longest\''
        
            return input_filename
        else:
            return

    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
        
    def _end_fit(self, fit_dict):
        
        '''
        _end_fit(fit_dict):
        
        End fitting procedure, close tensorboard writer, save checkpoint and full loss store to csv.
        
        Arguments:
            fit_dict (fict): dictionary containing object necessary for fitting
        '''
        
        # Close tensorboard writer
        if fit_dict['use_tensorboard']:
            fit_dict['writer'].close()

        # Save full loss store to csv via pandas
        import pandas as pd
        df = pd.DataFrame(self.full_loss_store)
        df.to_csv(self.model_dir + 'loss.csv', index_label='epoch')
        
        # Save a final checkpoint
        self.save_checkpoint(force=True)
        
        # Print message
        print('...training complete.')

    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
        
    def _fit_one_epoch(self, fit_dict, health_check=False):
        
        '''
        _fit_one_epoch(fit_dict, health_check)
        
        Fit one epoch of training data and validate model parameters with validation data
        
        Arguments:
            - fit_dict (dict) : dict containing objects necessary for fitting
            - health_check (bool) : whether to perform health check
        '''
        start = time.time()

        self.train()

        # Train model. Compute model output and loss, backpropagate loss gradients, optimize parameters.
        train_loss, train_recon_loss, train_kl_loss = self._train_one_epoch(fit_dict = fit_dict,
                                                                            health_check = health_check)

        # Validate model. Same as training, but no history tracking, backprop, or optimization steps
        valid_loss, valid_recon_loss, valid_kl_loss = self._test_one_epoch(fit_dict=fit_dict)

        epoch_runtime = time.time() - start

        # Print Epoch Loss
        print('Epoch: %4d, Step: %5d, Losses [Train, Valid]: Total [%.2f, %.2f], Recon [%.2f, %.2f], KL [%.2f, %.2f], L2 %.2f, Runtime: %.4f secs'%(self.epochs+1, self.current_step, train_loss, valid_loss, train_recon_loss, valid_recon_loss, train_kl_loss, valid_kl_loss, float(self.l2_loss.data), epoch_runtime), flush=True)

        # Apply learning rate decay function
        if self.scheduler_on:
            self.apply_decay(train_loss)

        # Store loss
        self.train_loss_store.append(float(train_loss))
        self.valid_loss_store.append(float(valid_loss))

        self.full_loss_store['train_loss'][self.epochs]       = float(train_loss)
        self.full_loss_store['train_recon_loss'][self.epochs] = float(train_recon_loss)
        self.full_loss_store['train_kl_loss'][self.epochs]    = float(train_kl_loss)
        self.full_loss_store['valid_loss'][self.epochs]       = float(valid_loss)
        self.full_loss_store['valid_recon_loss'][self.epochs] = float(valid_recon_loss)
        self.full_loss_store['valid_kl_loss'][self.epochs]    = float(valid_kl_loss)
        self.full_loss_store['l2_loss'][self.epochs]          = float(self.l2_loss.data)

        if fit_dict['use_tensorboard']:
            self._write_loss(fit_dict['writer'])

        self.epochs += 1

    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    def _start_epoch(self):
        '''
        _start_epoch()
        
        Routine for starting training epoch
        '''
        pass
    
    
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    def _end_epoch(self, fit_dict):
        '''
        _end_epoch(fit_dict):
        
        End current epoch by plotting and saving checkpoints
        '''
        
        if self.current_step >= max(self.cost_weights['kl']['schedule_start'] + self.cost_weights['kl']['schedule_dur'],
                                    self.cost_weights['l2']['schedule_start'] + self.cost_weights['l2']['schedule_dur']):
            if self.valid_loss_store[-1] < self.best:
                self.best = self.valid_loss_store[-1]
                # saving checkpoint
                start = time.time()
                self.save_checkpoint()
                print('Saving checkpoint: %.4f s taken'%(time.time()-start), flush=True)

                if fit_dict['use_tensorboard']:
                    figs_dict_train = self.plot_summary(data= fit_dict['train_dl'].dataset.tensors[0], truth= fit_dict['train_truth'])
                    figs_dict_valid = self.plot_summary(data= fit_dict['valid_dl'].dataset.tensors[0], truth= fit_dict['valid_truth'])

                    self._visualise_checkpoint_tensorboard(figs_dict_train=figs_dict_train,
                                                           figs_dict_valid=figs_dict_valid,
                                                           writer = fit_dict['writer'])
        
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def fit(self, train_dataset, valid_dataset,
            batch_size=4, max_epochs=100,
            use_tensorboard=True, health_check=False,
            train_truth=None, valid_truth=None, home_dir='.'):
        '''
        fit(self, train_dataset, valid_dataset, train_params=None, train_truth=None, valid_truth=None)        
        Fits the LFADS_Net using ADAM optimization.
        
        required arguments:
            - train_dataset (torch.utils.data.TensorDataset): Dataset with the training data to fit LFADS model
            - valid_dataset (torch.utils.data.TensorDataset): Dataset with validation data to validate LFADS model
        
        optional arguments:
            - batch_size (int) : number of data points in batch (default = 4)
            - max_epochs (int) : number of epochs to run in loop (default = 100)
            - use_tensorboard (bool) : whether to write results to tensorboard (default = False)
            - health_check (bool)    : whether to calculate weight and gradient norms
            - train_truth (torch.Tensor) : ground-truth rates for training dataset
            - valid_truth (torch.Tensor) : ground-truth rates for validation dataset
        '''
        
        fit_dict = self._prepare_fit(train_dataset= train_dataset, valid_dataset= valid_dataset,
                                    train_truth=train_truth, valid_truth = valid_truth,
                                    batch_size= batch_size, use_tensorboard= use_tensorboard,
                                    home_dir=home_dir)
        
        # for each epoch...
        for epoch in range(max_epochs):
            # If minimum learning rate reached, break training loop
            if self.learning_rate <= self.learning_rate_min:
                break
            self._start_epoch()
            self._fit_one_epoch(fit_dict= fit_dict, health_check=health_check)
            self._end_epoch(fit_dict=fit_dict)

        self._end_fit(fit_dict=fit_dict)
            
    #------------------------------------------------------------------------------W
    #------------------------------------------------------------------------------
    
    def plot_traces(self, pred, true, figsize=(8,8), num_traces=12, ncols=2, mode=None, norm=True, pred_logvar=None):
        '''
        Plot trace and compare to ground truth
        
        Arguments:
            - pred (np.array): array of predicted values to plot (dims: num_steps x num_cells)
            - true (np.array)   : array of true values to plot (dims: num_steps x num_cells)
            - figsize (2-tuple) : figure size (width, height) in inches (default = (8, 8))
            - num_traces (int)  : number of traces to plot (default = 24)
            - ncols (int)       : number of columns in figure (default = 2)
            - mode (string)     : mode to select subset of traces. Options: 'activity', 'rand', None.
                                  'Activity' plots the the num_traces/2 most active traces and num_traces/2
                                  least active traces defined sorted by mean value in trace
            - norm (bool)       : normalize predicted and actual values (default=True)
            - pred_logvar (np.array) : array of predicted values log-variance (dims: num_steps x num_cells) (default= None)
        
        '''
        
        num_cells = pred.shape[-1]
        
        nrows = int(num_traces/ncols)
        fig, axs = plt.subplots(figsize=figsize, nrows=nrows, ncols=ncols, sharex=True, sharey=True)
        axs = np.ravel(axs)
        
        if mode == 'rand':  
            idxs  = np.random.choice(list(range(num_cells)), size=num_traces, replace=False)
            idxs.sort()
                
        elif mode == 'activity':
            idxs = true.max(axis=0).argsort()[-num_traces:]
        
        else:
            idxs  = list(range(num_cells))
        
        time = np.arange(0, self.T*self.dt, self.dt)
        
        for ii, (ax,idx) in enumerate(zip(axs,idxs)):
            plt.sca(ax)
            plt.plot(time, true[:, idx], lw=2, color='#E84924')
            plt.plot(time, pred[:, idx], lw=2, color='#37A1D0')
            
#             # Hide the right and top spines
#             ax.spines['right'].set_visible(False)
#             ax.spines['top'].set_visible(False)
            
#             if ii >= num_traces - ncols:
#                 plt.xlabel('time (s)', fontsize=14)
#                 plt.xticks(fontsize=12)
#                 ax.xaxis.set_ticks_position('bottom')
                
#             else:
#                 plt.xticks([])
#                 ax.xaxis.set_ticks_position('none')
#                 ax.spines['bottom'].set_visible(False)

#             if ii%ncols==0:
#                 plt.yticks(fontsize=12)
#                 ax.yaxis.set_ticks_position('left')
#             else:
#                 plt.yticks([])
#                 ax.yaxis.set_ticks_position('none')
#                 ax.spines['left'].set_visible(False)
                
        fig.subplots_adjust(wspace=0.1, hspace=0.1)
        plt.legend(['Actual', 'Reconstructed'])
        
        return fig
    
    #------------------------------------------------------------------------------
    
    def plot_factors(self, max_in_col=5, figsize=(8,8)):
        
        '''
        plot_factors(max_in_col=5, figsize=(8,8))
        
        Plot inferred factors in a grid
        
        Arguments:
            - max_in_col (int) : maximum number of subplots in a column
            - figsize (tuple of 2 ints) : figure size in inches
        Returns
            - figure
        '''
        
        nrows = max_in_col
        ncols = int(np.ceil(self.factors_dim/max_in_col))
        
        fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)

        axs = np.ravel(axs)
        time = np.arange(0, self.T*self.dt, self.dt)
        factors = self.factors.mean(dim=0).cpu().numpy()
        fmin = factors.min()
        fmax = factors.max()
        
        for jx in range(self.factors_dim):
            plt.sca(axs[jx])
            plt.plot(time, factors[:, jx])
            plt.ylim(fmin-0.1, fmax+0.1)
            
            if jx%ncols == 0:
                plt.ylabel('Activity')
            else:
                plt.ylabel('')
                axs[jx].set_yticklabels([])
            
            if (jx - jx%ncols)/ncols == (nrows-1):
                plt.xlabel('Time (s)')
            else:
                plt.xlabel('')
                axs[jx].set_xticklabels([])
        
        fig.suptitle('Factors 1-%i for a sampled trial.'%factors.shape[1])
        fig.subplots_adjust(wspace=0.1, hspace=0.1)
        
        return fig
    
    #------------------------------------------------------------------------------
    
    def plot_inputs(self, fig_width=8, fig_height=1.5):
        
        '''
        plot_inputs(fig_width=8, fig_height=1.5)
        
        Plot inferred inputs
        
        Arguments:
            - fig_width (int) : figure width in inches
            - fig_height (int) : figure height in inches
        '''
    
        figsize = (fig_width, fig_height*self.u_dim)
        fig, axs = plt.subplots(nrows=self.u_dim, figsize=figsize)
        fig.suptitle('Input to the generator for a sampled trial', y=1.2)
        inputs = self.inputs_mean.mean(dim=0).cpu().numpy()
        time = np.arange(0, self.T*self.dt, self.dt)
        for jx in range(self.u_dim):
            if self.u_dim > 1:
                plt.sca(axs[jx])
            else:
                plt.sca(axs)
            plt.plot(time, inputs[:, jx])
            plt.xlabel('time (s)')
        return fig
    
    #------------------------------------------------------------------------------
    
    def compute_rsquared(self, x, y, model=None):
        
        '''TBC'''
        
        if L:
            return model(x, y).score()
        else:
            coefs = np.polyfit(x, y, deg=1)
            yhat  = np.poly1d(coefs)(x)
            ybar  = np.ravel(y).mean()
            ssr   = np.sum((yhat - ybar)**2)
            sst   = np.sum((y - ybar)**2)
            return ssr/sst
        
        
    #------------------------------------------------------------------------------
    
    def plot_3d(self, X, Y=None, figsize = (12, 12), view = (0, 0), title=None):
        
        '''TBC'''
        
        assert X.shape[0] == 3, 'X data must be 3 dimensional'
        if Y:
            assert Y.shape[0] == 3, 'Y data must be 3 dimensional'
            
        fig = plt.figure(figsize=figsize)
        ax = fig.add_subplot(111, projection='3d')
        ax.plot(X, lw=0.1)
        ax.plot(Y, lw=0.1, alpha=0.7)
        ax.view_init(view[0], view[1])
        if title:
            ax.set_title(title, fontsize=16)
        for legobj in leg.legendHandles:
            legobj.set_linewidth(2.0)
    
    #------------------------------------------------------------------------------
    
    def plot_summary(self, data, truth=None, num_average=100, ix=None):
        
        '''
        plot_summary(data, truth=None, num_average=100, ix=None)
        
        Plot summary figures for dataset and ground truth if available. Create a batch
        from one sample by repeating a certain number of times, and average across them.
        
        Arguments:
            - data (torch.Tensor) : dataset
            - truth (dict) : ground truth dictionary
            - num_average (int) : number of samples from posterior to average over
            - ix (int) : index of data samples to make summary plot from
            
        Returns:
            - fig_dict : dict of summary figures
        '''
        
        plt.close()
        
        figs_dict = {}
        
        batch_example, ix = batchify_random_sample(data=data, batch_size=num_average, ix=ix)
        figs_dict['ix'] = ix        
        
        pred_data = self.reconstruct(batch_example)
        true_data = data[ix].cpu().numpy()
        figs_dict['traces'] = self.plot_traces(pred_data, true_data, mode='activity', norm=False)
        figs_dict['traces'].suptitle('Actual fluorescence trace vs.\nestimated mean for a sampled trial')
        
        if truth:
            if 'rates' in truth.keys():
                pred_rates = self.rates.mean(dim=0).cpu().numpy()
                true_rates = truth['rates'][ix]
                figs_dict['truth_rates'] = self.plot_traces(pred_rates, true_rates, mode='rand')
                figs_dict['truth_rates'].suptitle('Estimated vs ground-truth rate function')
            
            if 'latent' in truth.keys():
                pred_factors = self.factors.mean(dim=0).cpu().numpy()
                true_factors = truth['latent'][ix]
                figs_dict['truth_factors'] = self.plot_traces(pred_factors, true_factors, num_traces=self.factors_dim, ncols=1)
                figs_dict['truth_factors'].suptitle('Estimated vs ground-truth factors')
            else:
                figs_dict['factors'] = self.plot_factors()
        
        else:
            figs_dict['factors'] = self.plot_factors()
        
        if self.u_dim > 0:
            figs_dict['inputs'] = self.plot_inputs()
        return figs_dict
    
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def plot_rsquared(self, x, y, figsize=(4,4), ms=1, title=''):
        
        '''
        TBC
        '''
        
        fig = plt.figure(figsize=figsize)
        plt.plot(x.flatten(), y.flatten(), '.', ms=ms, color='dimgrey', rasterized=True)
        plt.title(title, fontsize=16)
        plt.xlabel('Reconstruction')
        plt.ylabel('Truth')
        
        return fig

    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------

    def plot_recon_rsquared(self, data, truth, train_data=None, train_truth=None):
        
        results_dict = self.sample_and_average(data)
        if 'rates' in truth.keys():
            results_dict['rates']['rsq'] = r_squared(results_dict['rates']['average'].flatten().reshape(-1, 1), truth['rates'].flatten().reshape(-1, 1))[0]
            results_dict['rates']['fig'] = self.plot_rsquared(results_dict['rates']['average'], truth['rates'], title = 'Goodness of fit: rsq = %.3f'%results_dict['rates']['rsq'], ms=0.5)
            
        if 'latent' in truth.keys():
            results_dict['factors']['aligned'] = self._align_factors(train_data=train_data, train_truth=train_truth, factors=results_dict['factors']['average'])
            results_dict['factors']['rsq'] = r_squared(results_dict['factors']['aligned'].flatten().reshape(-1, 1), truth['latent'].flatten().reshape(-1, 1))[0]
            results_dict['factors']['fig'] = self.plot_rsquared(results_dict['factors']['aligned'], truth['latent'], title = 'Goodness of fit: rsq = %.3f'%results_dict['factors']['rsq'], ms=0.5)
            
        return results_dict
    
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def sample_and_average(self, x, num_average=100):
        rates = 0
        factors = 0
        for ix in range(num_average):
            self.forward_no_grad(x)
            rates += self.rates.cpu().numpy() / num_average
            factors += self.factors.cpu().numpy() / num_average
        
        return {'rates' : {'average' : rates},
                'factors' : {'average' : factors}}
    
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def _align_factors(self, train_data, train_truth, factors, num_average=100):
        factors_train = 0
        for ix in range(num_average):
            self.forward_no_grad(train_data)
            factors_train += self.factors.cpu().numpy() / num_average
        L = fit_linear_model(np.concatenate(factors_train), np.concatenate(train_truth['latent']))
        return L.predict(np.concatenate(factors))

    
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------

    def health_check(self, writer):
        '''
        Gets gradient norms for each parameter and writes to tensorboard
        '''
        
        for ix, (name, param) in enumerate(self.named_parameters()):
            if param.grad is not None:
                writer.add_scalar('3_Gradient_norms/%i_%s'%(ix, name), param.grad.data.norm(), self.current_step)
            else:
                writer.add_scalar('3_Gradient_norms/%i_%s'%(ix, name), 0.0, self.current_step)
    
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def _set_params(self, params):
        '''
        _set_params(params)
        
        Sets dictionary params to attributes
        '''
        for k in params.keys():
            self.__setattr__(k, params[k])

    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def _update_params(self, prev_params, new_params):
        '''
        _update_params(prev_params, new_params)
        
        
        '''
        if new_params:
            params = update_param_dict(prev_params, new_params)
        else:
            params = prev_params
        self._set_params(params)
        self.hyperparameters = params

#------------------------------------------------------------------------------
#------------------------------------------------------------------------------
#------------------------------------------------------------------------------
#------------------------------------------------------------------------------
#------------------------------------------------------------------------------
#------------------------------------------------------------------------------

#-----------------------
# LADDER LFADS EXTENSION
#-----------------------
        
class LadderLFADS(LFADS):
    
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    def __init__(self, inputs_dim, T, dt,
                 model_hyperparams=None,
                 device = 'cpu', save_variables=False,
                 seed=None):
        '''
        LFADS_Net (Latent Factor Analysis via Dynamical Systems) neural network class.
        
        __init__(self, inputs_dim, T, dt,
                 model_hyperparams=None,
                 device='cpu', save_variables=False)
                 
            required arguments:
            
            - inputs_dim (int): the dimensionality of the data (e.g. number of cells)
            - T (int): number of time-steps in one sequence (i.e. one data point)
            - dt (float): time-step in seconds
            
            optional arguments:
            - model_hyperparams (dict) : dictionary of model_hyperparameters
                - ### DATA HYPERPARAMETERS ### 
                - datatype (string) : datatype (default = 'calcium')
                - dataset_name (String): name given to identify dataset (default = 'unknown')
                - run_name (String): name given to identify model run (default = 'tmp')
                
                - ### MODEL DIMENSIONS ###
                - g_dim (int): dimensionality of the generator (default = 100)
                - u_dim (int): dimensionality of the inferred inputs to the generator (default = 1)
                - factors_dim (int): dimensionality of the latent factors (default = 20)
                - g0_encoder_dim (int): dimensionality of the encoder for the initial conditions for the generator
                                        (default = 100)
                - h0_encoder_dim (int): dimensionality of the encoder for the initial conditions of the dFF signal                
                - c_encoder_dim (int): dimensionality of the encoder for the controller (default = 100)
                - a_encoder_dim (int): dimensionality of the encoder for the amplitudes (default = 100)
                - controller_dim (int): dimensionality of controller (default = 100)
                
                - ### LATENT VARIABLE HYPERPARAMETERS ###
                - g0_prior_kappa (float) : initial variance for the learnable prior over the initial 
                                             generator state (default = 0.1)
                - g0_prior_var_min (float) : minimum variance for learnable prior over initial generator state.
                                             If none, set to g0_prior_kappa (default=None)
                - g0_prior_var_max (float) : maximum variance for learnable prior over initial generator state.
                                             If none, set to g0_prior_kappa (default=None)
                - g0_post_var_min (float) : minimum variance for posterior distribution over initial generator state.
                                            (default=0.0001)
                - g0_post_var_max (float) : maximum variance for posterior distribution over initial generator state.
                                            (default=np.inf)
                                             
                - h0_prior_kappa (float) : initial variance for the learnable prior over the initial observed
                                           fluorescence signal
                - h0_prior_var_min (float) : minimum variance for learnable prior over initial observed fluorescence
                                             signal. If none, set to y0_prior_kappa (default=None)
                - h0_prior_var_max (float) : maximum variance for learnable prior over initial observed fluorescence
                                             signal. If none, set to y0_prior_kappa (default=None)
                - h0_post_var_min (float) : minimum variance for posterior distribution over initial observed 
                                            fluorescence signal. (default=0.0001)
                - h0_post_var_max (float) : maximum variance for posterior distribution over initial observed 
                                            fluorescence signal. (default=np.inf)
                                             
                - u_prior_kappa (float): initial variance for the leanable prior over the inferred inputs
                                            to generator (default = 0.1)
                - u_prior_var_min (float) : minimum variance for learnable prior over inferred inputs.
                                             If none, set to u_prior_kappa (default=None)
                - u_prior_var_max (float) : maximum variance for learnable prior over inferred inputs.
                                             If none, set to u_prior_kappa (default=None)
                - u_prior_tau (float) : initial auto-regressive parameter for inferred inputs to generator (default=10)
                - u_prior_tau_min (float) : minimum  AR decay parameter for inferred inputs to generator (default = 0.1)
                - u_prior_tau_max (float) : maximum AR decay parameter for inferred inputs to generator (default = np.inf)
                - u_post_var_min (float) : minimum variance for posterior distribution over inferred inputs
                                           (default=0.0001)
                - u_post_var_max (float) : maximum variance for posterior distribution over inferred inputs 
                                           (default=np.inf)
                                             
                - h_prior_kappa (float): initial log-variance for the learnable prior over the inferred amplitudes
                                            in fluorescence signal (default = 0.1)
                - h_prior_var_min (float) : minimum variance for learnable prior over inferred amplitudes in fluorescence
                                            signal. If none, set to a_prior_kappa (default = None)
                - h_prior_var_max (float) : maximum variance for learnable prior over inferred amplitudes in fluorescence
                                            signal. If none, set to a_prior_kappa (default = None)
                - h_post_var_min (float) : minimum variance for posterior distribution over inferred amplitudes in 
                                           fluorescence signal (default=0.0001)
                - h_post_var_max (float) : maximum variance for posterior distribution over inferred amplitudes in
                                           fluorescence signal (default=np.inf)
                
                - ### OBSERVATION PARAMETERS ###
                - y_obs_var_init (float) : initial estimate of observation noise (default = 0.1)
                - y_obs_prior_var_init (float) : initial prior of observation noise (default = 0.1)
                
                - ### MISC HYPERPARAMETERS ###
                - keep_prob (float): keep probability for drop-out layers, if < 1 (default = 1.0)
                - clip_val (float): clips the hidden unit activity to be less than this value (default = 5.0)
                
                - ### OPTIMIZER HYPERPARAMETERS ###
                - lr (float): learning rate for ADAM optimizer (default = 0.01)
                - eps (float): epsilon value for ADAM optimizer (default = 0.1)
                - betas (2-tuple of floats): beta values for ADAM optimizer (default = (0.9, 0.999))
                - lr_decay (float): learning rate decay factor (default = 0.95)
                - lr_min (float): minimum learning rate (default = 1e-5)
                - max_norm (float): maximum gradient norm (default=200.0)
                - scheduler_on (bool): apply scheduler if True (default = True)
                - scheduler_patience (int): number of steps without loss decrease before weight decay (default = 6)
                - scheduler_cooldown (int): number of steps after weight decay to wait before next weight decay (default = 6)
                - kl_weight_min (float) : minimum weight on KL cost (default = 0.0)
                - kl_weight_schedule_start (int) : optimisation step to start kl_weight increase (default = 0)
                - kl_weight_schedule_dur (int) : number of optimisation steps to increase kl_weight to 1.0 (default = 2000)
                - l2_weight_min (float) : minimum weight on L2 cost (default = 0.0)
                - l2_weight_schedule_start (int) : optimisation step to start l2_weight increase (default = 0)
                - l2_weight_schedule_dur (int) : number of optimisation steps to increase l2_weight to 1.0 (default = 2000)
                - l2_gen_scale (float) : scaling factor for regularising l2 norm of generator hidden weights  (default = 0.0)
                - l2_con_scale (float) : scaling factor for regularising l2 norm of controller hidden weights (default = 0.0)
            
                - device (String): device to use (default= 'cpu')
                - save_variables (bool) : whether to save dynamic variables (default= False)
        '''
        
        # -----------------------
        # BASIC INIT STUFF
        # -----------------------
        
        # call the nn.Modules constructor
        super(LadderLFADS, self).__init__(inputs_dim=inputs_dim, T=T, dt=dt, 
                                          model_hyperparams=model_hyperparams,
                                          device = device,
                                          save_variables=save_variables,
                                          seed=seed)
        
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def _get_default_hyperparameters(self):
        
        '''
        _get_default_hyperparameters():
        
        Retrieve default hyperparameters for network
        
        Returns:
            - default_hyperparams (dict) : dictionary of default hyperparameters
        '''        

        # Default hyperparameters
        default_hyperparams  = {### DATA HYPERPARAMETERS ###
                                'datatype'                 : 'calcium',
                                'dataset_name'             : 'unknown',
                                'run_name'                 : 'tmp',
                                
                                ### MODEL HYPERPARAMETERS ### 
                                'g0_dim'                   : 100,
                                'g_dim'                    : 100,
                                'u_dim'                    : 1,
                                'h_dim'                    : 100,
                                'factors_dim'              : 20,
                                'g0_encoder_dim'           : 100,
                                'h0_encoder_dim'           : 100,
                                'c_encoder_dim'            : 100,
                                'a_encoder_dim'            : 100,
                                'c_controller_dim'         : 100,
                                'a_controller_dim'         : 100,
                                'g0_prior_kappa'           : 0.1,
                                'g0_prior_var_min'         : None,
                                'g0_prior_var_max'         : None,
                                'g0_post_var_min'          : 0.0001,
                                'g0_post_var_max'          : float('inf'),
                                'h0_prior_kappa'           : 0.1,
                                'h0_prior_var_min'         : None,
                                'h0_prior_var_max'         : None,
                                'h0_post_var_min'          : 0.0001,
                                'h0_post_var_max'          : float('inf'),
                                'u_prior_kappa'            : 0.1,
                                'u_prior_tau'              : 10,
                                'u_prior_tau_min'          : 0.1,
                                'u_prior_tau_max'          : float('inf'),
                                'u_prior_var_min'          : None,
                                'u_prior_var_max'          : None,
                                'u_post_var_min'           : 0.0001,
                                'u_post_var_max'           : float('inf'), 
                                'keep_prob'                : 1.0,
                                'h_prior_kappa'            : 0.1,
                                'h_prior_var_min'          : None,
                                'h_prior_var_max'          : None,
                                'h_post_var_min'           : 0.0001,
                                'h_post_var_max'           : float('inf'),
                                
                                ### MISCELLANEOUS HYPERPARAMETERS ###
                                'clip_val'                 : 5.0,
                                'norm_factors'             : True,
                                'deep_freeze'              : True,
                                'encoder_skip'             : False,
            
                                ### OBSERVATION HYPERPARAMETERS ###
                                'y_obs_var_init'           : 0.1,
                                'y_obs_prior_var_init'     : 0.1,
                                'y_tau_init'               : 0.1,
            
                                ### OPTIMIZER HYPERPARAMETERS ###
                                'learning_rate'            : 0.01,
                                'learning_rate_min'        : 1e-5,
                                'learning_rate_decay'      : 0.95,
                                'max_norm'                 : 200,
                                'scheduler_on'             : True,
                                'scheduler_patience'       : 6,
                                'scheduler_cooldown'       : 6,
                                'epsilon'                  : 0.1,
                                'betas'                    : (0.9, 0.99),
                                'l2_gen_scale'             : 0.0,
                                'l2_con_scale'             : 0.0,
                                'l1_scale'                 : 0.0,
                                'use_weight_schedule_fn'   : True,
                                'kl_weight_min'            : 0.0,
                                'kl_weight_schedule_start' : 0,
                                'kl_weight_schedule_dur'   : 2000,
                                'kl_deep_weight_min'       : 0.0,
                                'kl_deep_weight_schedule_start' : 0,
                                'kl_deep_weight_schedule_dur'   : 2000,
                                'l2_weight_min'            : 0.0,
                                'l2_weight_schedule_start' : 0,
                                'l2_weight_schedule_dur'   : 2000,
                                'l1_weight_min'            : 0.0,
                                'l1_weight_schedule_start' : 0,
                                'l1_weight_schedule_dur'   : 2000,}
        
        return default_hyperparams
    
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    def _set_hyperparameters(self):
        '''
        _set_hyperparameters():
        
        Set network hyperparameters
        '''
        
        super(LadderLFADS, self)._set_hyperparameters()
        
        # Cost function weight dictionary
        self.cost_weights = {'kl' : {'weight': self.kl_weight_min,
                                     'weight_min' : self.kl_weight_min,
                                     'schedule_start': self.kl_weight_schedule_start,
                                     'schedule_dur': self.kl_weight_schedule_dur},
                             'kl_deep' : {'weight': self.kl_deep_weight_min,
                                          'weight_min' : self.kl_deep_weight_min,
                                          'schedule_start': self.kl_deep_weight_schedule_start,
                                          'schedule_dur': self.kl_deep_weight_schedule_dur},
                             'l2' : {'weight': self.l2_weight_min,
                                     'weight_min' : self.l2_weight_min,
                                     'schedule_start': self.kl_weight_schedule_dur,
                                     'schedule_dur': self.l2_weight_schedule_dur},
                             'l1' : {'weight' : self.l1_weight_min,
                                     'weight_min' : self.l1_weight_min,
                                     'schedule_start' : self.l1_weight_schedule_start,
                                     'schedule_dur' : self.l1_weight_schedule_dur}}

    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    def _initialize_input_layers(self):
        
        # Bidirectional RNN - Encoder h0
        self.gru_Egen_h0 = nn.GRU(input_size=self.inputs_dim, hidden_size= self.h0_encoder_dim, bidirectional=True)
    
        # Bidirectional RNN - Encoder a
        self.gru_Econ_a = nn.GRU(input_size=self.inputs_dim, hidden_size= self.a_encoder_dim, bidirectional=True)
        
        
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    def _initialize_encoder_net(self):
        '''
        _initialize_encoder_net():
        
        Initialize modules for encoder network
        '''
        
        # Notation:
        #
        #   layertype_outputvariable(_direction)
        #
        #   Examples: fc_factors= "fully connected layer, variable = factors"
        #             gru_Egen_forward = "gated recurrent unit layer, encoder for generator, forward direction"
        # ---
        
        # Initialize LFADS encoder modules with requires_grad=False to keep weights frozen
        super(LadderLFADS, self)._initialize_encoder_net(requires_grad=False)
        
        # Bidirectional RNN - Encoder g0
        gru_Egen_g0_input_dim = self.h0_encoder_dim * 2 + self.inputs_dim if self.encoder_skip else self.h0_encoder_dim * 2
        self.gru_Egen_g0 = nn.GRU(input_size=gru_Egen_g0_input_dim, hidden_size= self.g0_encoder_dim, bidirectional=True)
        apply_requires_grad(module=self.gru_Egen_g0, requires_grad=False)
        
        if self.u_dim > 0: 
            
            # Bidirectional RNN - Encoder c
            gru_Econ_c_input_dim     = self.a_encoder_dim * 2 + self.inputs_dim if self.encoder_skip else self.a_encoder_dim * 2
            self.gru_Econ_c          = nn.GRU(input_size= self.a_encoder_dim * 2 + self.inputs_dim, hidden_size= self.c_encoder_dim, bidirectional=True)
            apply_requires_grad(module=self.gru_Econ_c, requires_grad=False)
        
        
        # Controller RNN a
        self.gru_controller_a = LFADS_GenGRUCell(input_size = self.a_encoder_dim * 2 + self.inputs_dim, hidden_size = self.a_controller_dim)
        
        # ----
        # Learnable RNN initial hidden states
        # ----
                
        # Generator forward encoder initial hidden state
        self.efgen_h0_init = nn.Parameter(torch.zeros(self.h0_encoder_dim))
        
        # Generator backward encoder initial hidden state
        self.ebgen_h0_init = nn.Parameter(torch.zeros(self.h0_encoder_dim))
        
        # Controller forward encoder initial hidden state
        self.efcon_a_init = nn.Parameter(torch.zeros(self.a_encoder_dim))

        # Controller backward encoder initial hidden state
        self.ebcon_a_init = nn.Parameter(torch.zeros(self.a_encoder_dim))
        
        # Fluorescence amplitude controller initial hidden state
        self.a_init = nn.Parameter(torch.zeros(self.a_controller_dim))
        
        
        # -----------
        # Fully connected layers
        # -----------
        
        # mean and logvar of the posterior distribution for the fluorescence signal initial conditions (h0 from E_gen_h0)
        # takes as inputs:
        #  - the forward encoder for h0 at time T (y0_enc_f_T)
        #  - the backward encoder for h0 at time 1 (y0_enc_b_0]
        self.fc_h0mean   = nn.Linear(in_features= 2 * self.h0_encoder_dim, out_features= self.h_dim)
        self.fc_h0logvar = nn.Linear(in_features= 2 * self.h0_encoder_dim, out_features= self.h_dim)
        
        
        # mean and logvar of the posterior distribution for the fluorescence signal amplitude generator (h provided to v)
        # takes as inputs:
        #  - the controller for a at time t (a_t)
        self.fc_hmean   = nn.Linear(in_features= self.a_controller_dim, out_features= self.h_dim)
        self.fc_hlogvar = nn.Linear(in_features= self.a_controller_dim, out_features= self.h_dim)
        
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def _initialize_generator_net(self):
        '''
        _initialize_generator_net()

        Initialize generator network
        '''
        super(LadderLFADS, self)._initialize_generator_net(requires_grad=False)
        
        # amplitudes from a samples
        self.fc_amps = nn.Linear(in_features=self.factors_dim + self.h_dim, out_features = self.inputs_dim)
        
        # Observation parameters
        self.y_obs_logvar = nn.Parameter(torch.ones(self.inputs_dim) * log(self.y_obs_var_init))
        self.y_obs_prior_logvar = nn.Parameter(torch.ones(self.inputs_dim) * log(self.y_obs_prior_var_init))
#         self.y_tau = nn.Parameter(torch.ones(self.inputs_dim) * self.y_tau_init)
        self.y_tau = torch.ones(self.inputs_dim).to(self.device) * 0.4
        
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def _initialize_priors(self):
        '''
        _initialize_priors()
        
        Initialize parameters of prior distributions
        '''
        
        # -------------------------------
        # LEARNABLE PRIOR PARAMETERS INIT
        # -------------------------------
        
        super(LadderLFADS, self)._initialize_priors(requires_grad=False)
        
        # Set prior mean for fluorescence signal initial conditions y0
        self.h0_prior_mu          = nn.Parameter(torch.zeros(self.h_dim))
        
        # Check if prior variance for initial conditions has a minimum and maximum hyperparameter,
        # if not, set to be same as inital prior variance
        if self.h0_prior_var_min is None:
            self.h0_prior_var_min = self.h0_prior_kappa
        if self.h0_prior_var_max is None:
            self.h0_prior_var_max = self.h0_prior_kappa
        
        # if prior variance minimum and maximum are the same, set to be fixed throughout training
        if np.isclose(self.h0_prior_var_min, self.h0_prior_var_max):
            self.h0_prior_logkappa = torch.ones(self.h_dim).to(self.device) * log(self.h0_prior_kappa)
            
        # if prior variance minimum and maximum are different, set to be a trainable parameter
        else:
            self.v0_prior_logkappa = nn.Parameter(torch.ones(self.h_dim)  * log(self.h0_prior_kappa))
        
        # Set prior mean for inputs
        self.h_prior_mu       = nn.Parameter(torch.zeros(self.h_dim))

        # Check if prior variance for amplitudes has a minimum and maximum hyperparameter,
        # if not, set to be same as inital prior variance
        if self.h_prior_var_min is None:
            self.h_prior_var_min = self.h_prior_kappa
        if self.h_prior_var_max is None:
            self.h_prior_var_max = self.h_prior_kappa

        # if prior variance minimum and maximum are the same, set to be fixed throughout training
        if np.isclose(self.h_prior_var_min, self.h_prior_var_max):
            self.h_prior_logkappa = torch.ones(self.h_dim).to(self.device) * log(self.h_prior_kappa)

        # if prior variance minimum and maximum are different, set to be a trainable parameter
        else:
            self.h_prior_logkappa = nn.Parameter(torch.ones(self.h_dim) * log(self.h_prior_kappa))
        
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def _initialize_optimizer(self):
        
        '''
        Initialize optimizer, set log-likelihood function, and freeze weights
        '''
        
        # Set log-likelihood function
        self.logLikelihood = logLikelihoodGaussian
        
        # OPTIMIZER INIT WITHOUT FROZEN WEIGHTS
        self.optimizer = opt.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=self.learning_rate, eps=self.epsilon, betas=self.betas)
        
        # Collect frozen parameters in parameter list:
        self.deep_params = nn.ParameterList([p for p in self.parameters() if not p.requires_grad])
        
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def initialize_variables(self, batch_size=None):
        '''
        initialize()
        
        Initialize dynamic model variables. These need to be reinitialized with each forward pass to
        ensure we don't need to retain graph between each .backward() call. 
        
        See https://discuss.pytorch.org/t/what-exactly-does-retain-variables-true-in-loss-backward-do/3508/2
        for discussion and explanation
        
        Note: The T + 1 terms  accommodate learnable biases for all variables, except for the generator,
        which is provided with a g0 estimate from the network
        
        optional arguments:
          batch_size (int) : batch dimension. If None, use self.batch_size.
        
        '''
        
        super(LadderLFADS, self).initialize_variables(batch_size=batch_size)
                
        # Initialise initial condition prior mean with new batch dimension
        self.h0_prior_mean   = torch.ones(batch_size, self.h_dim).to(self.device) * self.h0_prior_mu          # g0 prior mean
        
        # Initialise initial condition prior logvariance with new batch dimension
        self.h0_prior_logvar = torch.ones(batch_size, self.h_dim).to(self.device) * torch.clamp(self.h0_prior_logkappa, log(self.h0_prior_var_min), log(self.h0_prior_var_max))    # g0 prior logvar
        
        # Initialise latent space prior mean with new batch dimension
        self.h_prior_mean   = torch.ones(batch_size, self.h_dim).to(self.device) * self.h_prior_mu          # h prior mean
        
        # Initialise initial condition prior logvariance with new batch dimension
        self.h_prior_logvar = torch.ones(batch_size, self.h_dim).to(self.device) * torch.clamp(self.h_prior_logkappa, log(self.h_prior_var_min), log(self.h_prior_var_max))    # h prior logvar
        
        # Initialise generator encoder inital state with new batch dimension
        self.efgen_h0 = torch.ones((batch_size, self.h0_encoder_dim)).to(self.device) * self.efgen_h0_init  # Forward generator encoder
        self.ebgen_h0 = torch.ones((batch_size, self.h0_encoder_dim)).to(self.device) * self.ebgen_h0_init  # Backward generator encoder
        
        self.efcon_a = torch.ones((batch_size, self.a_encoder_dim)).to(self.device) * self.efcon_a_init  # Forward controller encoder
        self.ebcon_a = torch.ones((batch_size, self.a_encoder_dim)).to(self.device) * self.ebcon_a_init  # Backward controller encoder
        
        # Initialise controller state
        self.a = torch.ones((batch_size, self.a_controller_dim)).to(self.device) * self.a_init # Controller hidden state
            
        # Initialise state variables for storage
        if self.save_variables:
            self.amps          = torch.zeros(batch_size, self.T, self.inputs_dim)
            self.fluor        = torch.zeros(batch_size, self.T, self.inputs_dim)
            
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
            
    def encode(self, x):
        '''
        encode(x)
        
        Function to encode the data with the forward and backward encoders.
        
        Arguments:
          - x (torch.Tensor): Variable tensor of size batch size x time-steps x input dimension
        '''
        
        # Dropout some data
        if self.keep_prob < 1.0:
            x = self.dropout(x)
        
        out_gru_egen_h0, hidden_gru_egen_h0 = self.gru_Egen_h0(x, torch.stack((self.efgen_h0, self.ebgen_h0)))
        self.egen_h0_1T_dropout  = self.dropout(hidden_gru_egen_h0.clamp(min=-self.clip_val, max=self.clip_val))
        self.egen_h0_1T_dropout  = torch.cat((self.egen_h0_1T_dropout[0], self.egen_h0_1T_dropout[1]), dim=1)
        
        in_gru_egen_g0 = torch.cat((x, out_gru_egen_h0.clamp(min=-self.clip_val, max=self.clip_val)), dim=2) if self.encoder_skip else out_gru_egen_h0.clamp(min=-self.clip_val, max=self.clip_val)
        out_gru_egen_g0, hidden_gru_egen_g0 = self.gru_Egen_g0(in_gru_egen_g0, torch.stack((self.efgen_g0, self.ebgen_g0)))
        self.egen_g0_1T_dropout  = self.dropout(hidden_gru_egen_g0.clamp(min=-self.clip_val, max=self.clip_val))
        self.egen_g0_1T_dropout  = torch.cat((self.egen_g0_1T_dropout[0], self.egen_g0_1T_dropout[1]), dim=1)
        
        out_gru_econ_a, hidden_gru_econ_a = self.gru_Econ_a(x, torch.stack((self.efcon_a, self.ebcon_a)))
        self.econ_a_1tT = out_gru_econ_a.clamp(min=-self.clip_val, max=self.clip_val)
        
        if self.u_dim > 0:
            in_gru_econ_c = torch.cat((x, self.econ_a_1tT.clamp(min=-self.clip_val, max=self.clip_val)), dim=2) if self.encoder_skip else self.econ_a_1tT.clamp(min=-self.clip_val, max=self.clip_val)
            out_gru_econ_c, hidden_gru_econ_c = self.gru_Econ_c(in_gru_econ_c, torch.stack((self.efcon_c, self.ebcon_c)))
            self.econ_c_1tT = out_gru_econ_c.clamp(min=-self.clip_val, max=self.clip_val)
        
        self.h0_mean   = self.fc_h0mean(self.egen_h0_1T_dropout)
        self.h0_logvar = self.fc_h0logvar(self.egen_h0_1T_dropout)
        
        self.h         = Variable(torch.randn(self.batch_size, self.h_dim).to(self.device))*torch.exp(0.5 * self.h0_logvar) \
                         + self.h0_mean
        
        self.g0_mean   = self.fc_g0mean(self.egen_g0_1T_dropout)
        self.g0_logvar = self.fc_g0logvar(self.egen_g0_1T_dropout)
        self.g0         = Variable(torch.randn(self.batch_size, self.g0_dim).to(self.device))*torch.exp(0.5 * self.g0_logvar)\
                         + self.g0_mean
        
        self.g         = self.fc_icgen(self.g0)
        # Dropout some of the generator state for sending to factors, but keep generator state intact for feeding back to generator
        if self.keep_prob < 1.0:
            self.g_do      = self.dropout(self.g)
        
        # Initialise factors
        self.f         = self.fc_factors(self.g_do)
        
        # Update v
        h_and_fac = torch.cat((self.h, self.f), dim = 1)
        if self.keep_prob < 1.0:
            h_and_fac = self.dropout(h_and_fac)

        self.v = torch.clamp(self.fc_amps(h_and_fac).exp() - 1, min= 0.0)
        
        # Initialise Observation
        self.y         = self.v.clone()
        
        
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def generate_one_step(self, t):
        '''
        generate_one_step(t)
        
        Sample from approximate posterior, update generator and reconstruct input for one time step
        
        Arguments:
            - t (int) : time-step
        '''
        
        super(LadderLFADS, self).generate_one_step(t)
        # Concatenate ebcon_a and efcon_a outputs at time t with factors at time t-1 as input to controller for amplitudes
        econ_a_and_v = torch.cat((self.econ_a_1tT[t], self.v), dim= 1)

        if self.keep_prob < 1.0:
            econ_a_and_v = self.dropout(econ_a_and_v)

        # Update controller with controller encoder outputs
        self.a = torch.clamp(self.gru_controller_a(econ_a_and_v, self.a), min=-self.clip_val, max=self.clip_val)

        # Calculate posterior distribution parameters for inferred amplitudes from controller state

        self.h_mean   = self.fc_hmean(self.a)
        self.h_logvar = self.fc_hlogvar(self.a)
        self.h = Variable(torch.randn(self.batch_size, self.h_dim).to(self.device))*torch.exp(0.5*self.h_logvar) + self.h_mean

        # Update v
        h_and_fac = torch.cat((self.h, self.f), dim = 1)
        if self.keep_prob < 1.0:
            h_and_fac = self.dropout(h_and_fac)

        self.v = torch.clamp(self.fc_amps(h_and_fac).exp() - 1, min= 0.0)

        # Generate fluorescence trace
        self.y = self.y.clone()*(1-self.dt/self.y_tau) + self.v
        
        if self.save_variables and t >= 0:
            self.amps[:, t] = self.v.detach().cpu()
            self.fluor[:, t]   = self.y.detach().cpu()
        
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def compute_recon_loss_one_step(self, x, t):
        '''
        compute_recon_loss_one_step(x, t)
        
        Compute reconstruction loss for one time step and add to recon_loss of model
        
        Arguments:
            - x (torch.Tensor) : Single trial data
            - t (int) : time-step
        '''
        if t == -1:
            self.spike_recon_loss = 0
            self.recon_loss = 0
        else:        
            self.spike_recon_loss = self.spike_recon_loss - logLikelihoodPoisson(self.v, self.r*self.dt)/self.batch_size
            self.recon_loss = self.recon_loss - self.logLikelihood(x[t], self.y, self.y_obs_logvar)/self.batch_size
    
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def compute_kl_loss_one_step(self, t):
        '''
        compute_kl_loss_one_step(t):
        
        Compute kl loss for one time step and add to kl_loss of model
        
        Arguments:
            - t (int) : time-step
        '''
        if t == -1:
            # KL cost for g(0)
            self.kl_deep_loss = KLCostGaussian(self.g0_mean, self.g0_logvar,
                                               self.g0_prior_mean, self.g0_prior_logvar)/self.batch_size
            # KL cost for h(0)
            self.kl_loss   = KLCostGaussian(self.h0_mean, self.h0_logvar,
                                            self.h0_prior_mean, self.h0_prior_logvar)/self.batch_size
            
            self.kl_loss = self.kl_loss + KLCostGaussian(0.0, self.y_obs_logvar, 0.0, self.y_obs_prior_logvar)

        elif t == 0:
            self.kl_loss = self.kl_loss +  KLCostGaussian(self.h_mean, self.h_logvar,
                                           self.h_prior_mean, self.h_prior_logvar)/self.batch_size
            
            if self.u_dim > 0:
                self.kl_deep_loss = self.kl_deep_loss +  KLCostGaussian(self.u_mean, self.u_logvar,
                                                         self.u_prior_mean, self.u_prior_logvar)/self.batch_size
                
                self.u_prev = self.u
            
        else:
            self.kl_loss = self.kl_loss + KLCostGaussian(self.h_mean, self.h_logvar,
                                           self.h_prior_mean, self.h_prior_logvar)/self.batch_size
            
            if self.u_dim > 0:
                gp_prior_mean   = (self.u_prev - self.u_prior_mean) * torch.exp(-1/self.u_prior_tau) + self.u_prior_mean
                gp_prior_logvar = torch.log(1 - torch.exp(-1/self.u_prior_tau).pow(2)) + self.u_prior_logvar

                self.kl_deep_loss = self.kl_deep_loss + KLCostGaussian(self.u_mean, self.u_logvar,
                                                                       gp_prior_mean, gp_prior_logvar)/self.batch_size
                self.u_prev = self.u
                
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def compute_l1_loss_one_step(self, t):
        if t == -1:
            self.l1_loss = self.l1_scale * self.v.abs().sum()/self.batch_size
            
        else:
            self.l1_loss = self.l1_loss + self.l1_scale * self.v.abs().sum()/self.batch_size
            
    def compute_loss_one_step(self, x, t):
        super(LadderLFADS, self).compute_loss_one_step(x, t)
        self.compute_l1_loss_one_step(t)

    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
            
    def _write_loss(self, writer):
        '''
        _write_loss(writer)
        
        Write loss to tensorboard
        
        Arguments:
            - writer (tensorboardX.Writer) : writer for tensorboard
        '''
        super(LadderLFADS, self)._write_loss(writer)
        
        kl_deep_weight = self.cost_weights['kl_deep']['weight']
        writer.add_scalar('2_Optimizer/2.5_KL_deep_weight', kl_deep_weight, self.epochs)
        
        l1_weight = self.cost_weights['l1']['weight']
        writer.add_scalar('2_Optimizer/4_L1_weight', l1_weight, self.epochs)
        
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    def reconstruct(self, x):
        '''
        reconstruct(x)
        
        Runs a forward pass through the network, and outputs reconstruction of data x. History is not tracked.
        
        Arguments:
          - x (torch.Tensor): Single-trial spike data. Tensor of size batch size x time-steps x input dimensions
          
        Returns:
          - output (torch.Tensor): Reconstruction. Tensor of size batch size x time-steps x input dimensions
        '''
        
        self.forward_no_grad(x)
        return self.fluor.mean(dim=0).cpu().numpy()
    
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def _compute_total_loss_one_step(self, x):
        '''
        _compute_loss_one_step(x)
        
        Zero gradients in optimizer, forward pass of model, then collect loss terms.
        
        Arguments:
            - x (torch.Tensor) : Data batch
        '''
        
        self.current_step += 1

        # apply Variable wrapper to batch
        x = Variable(x[0])

        # zero the parameter gradients
        self.optimizer.zero_grad()

        # Update regularizer weights
        if self.use_weight_schedule_fn:
            self.weight_schedule_fn(self.current_step)

        # Forward
        self(x)

        # Calculate l2 regularisation penalty
        self.l2_loss = 0.5 * self.l2_gen_scale * self.gru_generator.hidden_weight_l2_norm()
        if self.u_dim > 0:
            self.l2_loss += 0.5 * self.l2_con_scale * self.gru_controller_c.hidden_weight_l2_norm()

        # Collect separate weighted losses
        kl_weight      = self.cost_weights['kl']['weight']
        l2_weight      = self.cost_weights['l2']['weight']
        l1_weight      = self.cost_weights['l1']['weight']
        kl_deep_weight = self.cost_weights['kl_deep']['weight']
        
        # Add deep losses
        self.kl_loss    += kl_deep_weight * self.kl_deep_loss
        self.recon_loss += self.spike_recon_loss

        # Compute loss with KL and L2 warmup
        loss = self.recon_loss + kl_weight * self.kl_loss + l2_weight * self.l2_loss + l1_weight * self.l1_loss
        
        return loss
    
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def _test_one_epoch(self, fit_dict):
        '''
        _test_one_epoch(fit_dict)
        
        Test model parameters on validation dataset
        
        Arguments:
            - fit_dict (dict) : Dictionary containing dataloaders, ground truth and other information
            
        Returns:
            - test_loss (float)
            - test_recon_loss (float)
            - test_kl_loss (float)
        '''
        self.eval()

        test_loss = 0
        test_recon_loss = 0
        test_kl_loss = 0
            
        for i, x in enumerate(fit_dict['valid_dl'], 0):
            with torch.no_grad():
                x = Variable(x[0])
                self(x)
                self.recon_loss = self.recon_loss + self.spike_recon_loss
                self.kl_loss    = self.kl_loss + self.kl_deep_loss
                loss = self.recon_loss + self.kl_loss + self.l2_loss + self.l1_loss
                test_loss += loss.data
                test_recon_loss += self.recon_loss.data
                test_kl_loss += self.kl_loss.data
                
                
        test_loss /= (i+1)
        test_recon_loss /= (i+1)
        test_kl_loss /= (i+1)
        return test_loss, test_recon_loss, test_kl_loss

    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------

    def _end_epoch(self, fit_dict):
        
        '''
        _end_epoch(fit_dict):
        
        End current epoch by plotting and saving checkpoints
        '''
        
        if self.current_step >= self.cost_weights['kl']['schedule_start'] + self.cost_weights['kl']['schedule_dur'] and self.deep_freeze:
            self.deep_freeze = False
            for p in self.deep_params:
                p.requires_grad = True
            self.optimizer.add_param_group({'params': self.deep_params})
            
        super(LadderLFADS, self)._end_epoch(fit_dict = fit_dict)
    
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def _visualise_checkpoint_tensorboard(self, figs_dict_train, figs_dict_valid, writer):
        
        '''
        _visualise_checkpoint_tensorboard(figs_dict_train, figs_dict_valid, writerS)
        
        Visualise checkpoint in tensorboard
        
        Arguments:
            - figs_dict_train (dict): dictionary containing figures of training data results
            - figs_dict_valid (dict): dictionary containing figures of validation data results
            - writer (tensorboardX.Writer): writer for tensorboard plotting
        '''
        
        super(LadderLFADS, self)._visualise_checkpoint_tensorboard(figs_dict_train, figs_dict_valid, writer)
            
        if 'truth_spikes' in figs_dict_train.keys():
            writer.add_figure('Ground_truth/1_Train/3_Spikes', figs_dict_train['truth_spikes'], self.epochs, close=True)
            
        if 'truth_spikes' in figs_dict_valid.keys():
            writer.add_figure('Ground_truth/2_Valid/3_Spikes', figs_dict_valid['truth_spikes'], self.epochs, close=True)

    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
            
    def plot_summary(self, data, truth=None, num_average=100, ix= None):
        '''
        plot_summary(data, truth=None, num_average=100, ix=None)
        
        Plot summary figures for dataset and ground truth if available. Create a batch
        from one sample by repeating a certain number of times, and average across them.
        
        Arguments:
            - data (torch.Tensor) : dataset
            - truth (dict) : ground truth dictionary
            - num_average (int) : number of samples from posterior to average over
            - ix (int) : index of data samples to make summary plot from
            
        Returns:
            - fig_dict : dict of summary figures
        '''
        figs_dict = super(LadderLFADS, self).plot_summary(data, truth=truth, num_average=num_average, ix=ix)
        ix = figs_dict['ix']
        if truth:
            if 'spikes' in truth.keys():
                pred_spikes = self.amps.mean(dim=0).cpu().numpy()
                true_spikes = truth['spikes'][ix]
                figs_dict['truth_spikes'] = self.plot_traces(pred_spikes, true_spikes, mode='rand')
                figs_dict['truth_spikes'].suptitle('Estimated vs ground-truth spikes')
        return figs_dict
            
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def _load_optimizer_checkpoint(self, opt_state):
        '''
        _load_optimizer_checkpoint(opt_state)
        
        load checkpoint for optimizer
        
        Arguments:
            - opt_state (dict) : optimizer state dict
        '''
        if self.current_step >= self.cost_weights['kl']['schedule_start'] + self.cost_weights['kl']['schedule_dur'] and self.deep_freeze:
            self.deep_freeze = False
            for p in self.deep_params:
                p.requires_grad = True
            self.optimizer.add_param_group({'params': self.deep_params})
            self.optimizer.load_state_dict(opt_state)
        else:
            super(LadderLFADS, self)._load_optimizer_checkpoint(opt_state)
    
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------

    def plot_recon_rsquared(self, data, truth, train_data=None, train_truth=None):
        
        results_dict = self.sample_and_average(data)
        if 'fluor' in truth.keys():
            results_dict['fluor']['rsq'] = r_squared(results_dict['fluor']['average'].flatten().reshape(-1, 1), truth['fluor'].flatten().reshape(-1, 1))[0]
            results_dict['fluor']['fig'] = self.plot_rsquared(results_dict['fluor']['average'], truth['fluor'], title = 'Goodness of fit: rsq = %.3f'%results_dict['fluor']['rsq'], ms=0.1)
        
        if 'spikes' in truth.keys():
            results_dict['spikes']['rsq'] = r_squared(results_dict['spikes']['average'].flatten().reshape(-1, 1), truth['spikes'].flatten().reshape(-1, 1))[0]
            results_dict['spikes']['fig'] = self.plot_rsquared(results_dict['spikes']['average'], truth['spikes'], title = 'Goodness of fit: rsq = %.3f'%results_dict['spikes']['rsq'], ms=0.1)
        
        if 'rates' in truth.keys():
            results_dict['rates']['rsq'] = r_squared(results_dict['rates']['average'].flatten().reshape(-1, 1), truth['rates'].flatten().reshape(-1, 1))[0]
            results_dict['rates']['fig'] = self.plot_rsquared(results_dict['rates']['average'], truth['rates'], title = 'Goodness of fit: rsq = %.3f'%results_dict['rates']['rsq'], ms=0.1)
            
        if 'latent' in truth.keys():
            results_dict['factors']['aligned'] = self._align_factors(train_data=train_data, train_truth=train_truth, factors=results_dict['factors']['average'])
            results_dict['factors']['rsq'] = r_squared(results_dict['factors']['aligned'].flatten().reshape(-1, 1), truth['latent'].flatten().reshape(-1, 1))[0]
            results_dict['factors']['fig'] = self.plot_rsquared(results_dict['factors']['aligned'], truth['latent'], title = 'Goodness of fit: rsq = %.3f'%results_dict['factors']['rsq'], ms=0.1)
            

            
        return results_dict
    
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def sample_and_average(self, x, num_average=100):
        rates = 0
        factors = 0
        fluor = 0
        spikes = 0
        for ix in range(num_average):
            self.forward_no_grad(x)
            rates   += self.rates.cpu().numpy() / num_average
            factors += self.factors.cpu().numpy() / num_average
            fluor   += self.fluor.cpu().numpy() / num_average
            spikes  += self.amps.cpu().numpy() / num_average
        
        return {'rates'   : {'average' : rates},
                'factors' : {'average' : factors},
                'fluor'   : {'average' : fluor},
                'spikes'  : {'average' : spikes}
               }
    
#------------------------------------------------------------------------------
#------------------------------------------------------------------------------
#------------------------------------------------------------------------------
#------------------------------------------------------------------------------
#------------------------------------------------------------------------------
#------------------------------------------------------------------------------

#-----------------------
# Moment LFADS EXTENSION
#-----------------------
    
class MomentLFADS(LFADS):
    
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def __init__(self, inputs_dim, T, dt,
                 model_hyperparams=None,
                 device = 'cpu', save_variables=False,
                 seed=None):
        
        '''
        MomentLFADS (Latent Factor Analysis via Dynamical Systems) neural network class.
        Reconstructs time-varying moments of calcium fluorescence signals.
        
        __init__(self, inputs_dim, T, dt,
                 model_hyperparams=None,
                 device='cpu', save_variables=False)
                 
            required arguments:
            
            - inputs_dim (int): the dimensionality of the data (e.g. number of cells)
            - T (int): number of time-steps in one sequence (i.e. one data point)
            - dt (float): time-step in seconds
            
            optional arguments:
            - model_hyperparams (dict) : dictionary of model_hyperparameters
                - ### DATA HYPERPARAMETERS ### 
                - datatype (string) : datatype (default = 'calcium')
                - dataset_name (String): name given to identify dataset (default = 'unknown')
                - run_name (String): name given to identify model run (default = 'tmp')
                
                - ### MODEL DIMENSIONS ###
                - g_dim (int): dimensionality of the generator (default = 100)
                - u_dim (int): dimensionality of the inferred inputs to the generator (default = 1)
                - factors_dim (int): dimensionality of the latent factors (default = 20)
                - g0_encoder_dim (int): dimensionality of the encoder for the initial conditions for the generator
                                        (default = 100)
                - c_encoder_dim (int): dimensionality of the encoder for the controller (default = 100)
                - controller_dim (int): dimensionality of controller (default = 100)
                
                - ### LATENT VARIABLE HYPERPARAMETERS ###
                - g0_prior_kappa (float) : initial variance for the learnable prior over the initial 
                                             generator state (default = 0.1)
                - g0_prior_var_min (float) : minimum variance for learnable prior over initial generator state.
                                             If none, set to g0_prior_kappa (default=None)
                - g0_prior_var_max (float) : maximum variance for learnable prior over initial generator state.
                                             If none, set to g0_prior_kappa (default=None)
                - g0_post_var_min (float) : minimum variance for posterior distribution over initial generator state.
                                            (default=0.0001)
                - g0_post_var_max (float) : maximum variance for posterior distribution over initial generator state.
                                            (default=np.inf)
                - u_prior_kappa (float): initial variance for the leanable prior over the inferred inputs
                                            to generator (default = 0.1)
                - u_prior_var_min (float) : minimum variance for learnable prior over inferred inputs.
                                             If none, set to u_prior_kappa (default=None)
                - u_prior_var_max (float) : maximum variance for learnable prior over inferred inputs.
                                             If none, set to u_prior_kappa (default=None)
                - u_prior_tau (float) : initial auto-regressive parameter for inferred inputs to generator (default=10)
                - u_prior_tau_min (float) : minimum  AR decay parameter for inferred inputs to generator (default = 0.1)
                - u_prior_tau_max (float) : maximum AR decay parameter for inferred inputs to generator (default = np.inf)
                - u_post_var_min (float) : minimum variance for posterior distribution over inferred inputs
                                           (default=0.0001)
                - u_post_var_max (float) : maximum variance for posterior distribution over inferred inputs 
                                           (default=np.inf)
                                                           
                - ### MISC HYPERPARAMETERS ###
                - keep_prob (float): keep probability for drop-out layers, if < 1 (default = 1.0)
                - clip_val (float): clips the hidden unit activity to be less than this value (default = 5.0)
                
                - ### OPTIMIZER HYPERPARAMETERS ###
                - lr (float): learning rate for ADAM optimizer (default = 0.01)
                - eps (float): epsilon value for ADAM optimizer (default = 0.1)
                - betas (2-tuple of floats): beta values for ADAM optimizer (default = (0.9, 0.999))
                - lr_decay (float): learning rate decay factor (default = 0.95)
                - lr_min (float): minimum learning rate (default = 1e-5)
                - max_norm (float): maximum gradient norm (default=200.0)
                - scheduler_on (bool): apply scheduler if True (default = True)
                - scheduler_patience (int): number of steps without loss decrease before weight decay (default = 6)
                - scheduler_cooldown (int): number of steps after weight decay to wait before next weight decay (default = 6)
                - kl_weight_min (float) : minimum weight on KL cost (default = 0.0)
                - kl_weight_schedule_start (int) : optimisation step to start kl_weight increase (default = 0)
                - kl_weight_schedule_dur (int) : number of optimisation steps to increase kl_weight to 1.0 (default = 2000)
                - l2_weight_min (float) : minimum weight on L2 cost (default = 0.0)
                - l2_weight_schedule_start (int) : optimisation step to start l2_weight increase (default = 0)
                - l2_weight_schedule_dur (int) : number of optimisation steps to increase l2_weight to 1.0 (default = 2000)
                - l2_gen_scale (float) : scaling factor for regularising l2 norm of generator hidden weights  (default = 0.0)
                - l2_con_scale (float) : scaling factor for regularising l2 norm of controller hidden weights (default = 0.0)
            
                - device (String): device to use (default= 'cpu')
                - save_variables (bool) : whether to save dynamic variables (default= False)
        '''
        
        super(MomentLFADS, self).__init__(inputs_dim=inputs_dim, T=T, dt=dt,
                                          model_hyperparams=model_hyperparams,
                                          device = device, save_variables=save_variables,
                                          seed=seed)

    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def _get_default_hyperparameters(self):
        '''
        _get_default_hyperparameters():
        
        Retrieve default hyperparameters for network
        
        Returns:
            - default_hyperparams (dict) : dictionary of default hyperparameters
        '''
        
        default_hyperparams  = {### DATA PARAMETERS ###
                                'datatype'                 : 'calcium',
                                'dataset_name'             : 'unknown',
                                'run_name'                 : 'tmp',
                                
                                ### MODEL PARAMETERS ### 
                                'g_dim'                    : 100,
                                'u_dim'                    : 1, 
                                'factors_dim'              : 20,
                                'kernel_dim'               : 20,
                                'g0_encoder_dim'           : 100,
                                'c_encoder_dim'            : 100,
                                'c_controller_dim'         : 100,
                                'g0_prior_kappa'           : 0.1,
                                'g0_prior_var_min'         : None,
                                'g0_prior_var_max'         : None,
                                'g0_post_var_min'          : 0.0001,
                                'g0_post_var_max'          : float('inf'),
                                'u_prior_kappa'            : 0.1,
                                'u_prior_tau'              : 10,
                                'u_prior_tau_min'          : None,
                                'u_prior_tau_max'          : float('inf'),
                                'u_prior_var_min'          : None,
                                'u_prior_var_max'          : None,
                                'u_post_var_min'           : 0.0001,
                                'u_post_var_max'           : float('inf'), 
                                'order'                    : 2,
                                'keep_prob'                : 1.0,
                                'clip_val'                 : 5.0,
                                'max_norm'                 : 200,
                                'norm_factors'             : True,
                                'clip_moments'             : False,
                                'reg_moments'              : False,
                                'infer_padding'            : False,
                                
                                ### OPTIMIZER PARAMETERS 
                                'learning_rate'            : 0.01,
                                'learning_rate_min'        : 1e-5,
                                'learning_rate_decay'      : 0.95,
                                'scheduler_on'             : True,
                                'scheduler_patience'       : 6,
                                'scheduler_cooldown'       : 6,
                                'epsilon'                  : 0.1,
                                'betas'                    : (0.9, 0.99),
                                'l2_gen_scale'             : 0.0,
                                'l2_con_scale'             : 0.0,
                                'use_weight_schedule_fn'   : True,
                                'kl_weight_min'            : 0.0,
                                'kl_weight_schedule_start' : 0,
                                'kl_weight_schedule_dur'   : 2000,
                                'l2_weight_min'            : 0.0,
                                'l2_weight_schedule_start' : 0,
                                'l2_weight_schedule_dur'   : 2000,
                                'ew_weight_min'            : 0.0,
                                'ew_weight_schedule_start' : 0,
                                'ew_weight_schedule_dur'   : 2000}
        
        return default_hyperparams

    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    def _set_hyperparameters(self):
        '''
        _set_hyperparameters():
        
        Set network hyperparameters
        '''
        
        super(MomentLFADS, self)._set_hyperparameters()
        self.cost_weights['ew'] = {'weight': self.ew_weight_min,
                                   'weight_min' : self.ew_weight_min,
                                   'schedule_start': self.ew_weight_schedule_start, 
                                   'schedule_dur': self.ew_weight_schedule_dur}
    
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------

    def _initialize_input_layers(self):
        '''
        _initialize_input_layers(required_grad=True)
        
        Initialize layers that receive data as input
        '''
        
        # Notation:
        #
        #   layertype_outputvariable(_direction)
        #
        #   Examples: fc_factors= "fully connected layer, variable = factors"
        #             gru_Egen_forward = "gated recurrent unit layer, encoder for generator, forward direction"
        
        # Generator Forward Encoder
        self.gru_Egen_g0_forward  = LFADS_GRUCell(input_size= self.inputs_dim, hidden_size= self.g0_encoder_dim)

        # Generator Backward Encoder
        self.gru_Egen_g0_backward = LFADS_GRUCell(input_size= self.inputs_dim, hidden_size= self.g0_encoder_dim)

        if self.u_dim > 0:
            # Controller Forward Encoder
            self.gru_Econ_c_forward  = LFADS_GRUCell(input_size= self.inputs_dim, hidden_size= self.c_encoder_dim)

            # Controller Backward Encoder
            self.gru_Econ_c_backward = LFADS_GRUCell(input_size= self.inputs_dim, hidden_size= self.c_encoder_dim)

    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------

    def _initialize_generator_net(self):
        '''
        _initialize_generator_net():
        
        Initialize modules for generator network
        '''
        
        # Notation:
        #
        #   layertype_outputvariable(_direction)
        #
        #   Examples: fc_factors= "fully connected layer, variable = factors"
        #             gru_Egen_forward = "gated recurrent unit layer, encoder for generator, forward direction"
            
        super(MomentLFADS, self)._initialize_generator_net()
        
        self.conv_fluormoments = CausalChannelConv1d(kernel_size=self.kernel_dim, out_channels= self.order, infer_padding= self.infer_padding)

    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
        
    def _initialize_weights(self):
        '''
        _initialize_weights()
        
        Initialize network weights
        '''
        
        super(MomentLFADS, self)._initialize_weights()
        
        for m in self.modules():
            if isinstance(m, CausalChannelConv1d):
                k = m.weight.shape[2]
                m.weight.data.uniform_(0.0, k**-0.5)

    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
                                
    def _initialize_optimizer(self):
        '''
        _initialize_optimizer()
        
        Initialize optimizer and likelihood function
        '''
        self.optimizer = opt.Adam(self.parameters(), lr=1e-8, eps=self.epsilon, betas=self.betas)

        self.logLikelihood = logLikelihoodEdgeworth
        
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------

    def initialize_variables(self, batch_size=None):
        '''
        initialize_variables(batch_size=None)
        
        Initialize dynamic model variables. These need to be reinitialized with each forward pass to
        ensure we don't need to retain graph between each .backward() call. 
        
        See https://discuss.pytorch.org/t/what-exactly-does-retain-variables-true-in-loss-backward-do/3508/2
        for discussion and explanation
        
        Note: The T + 1 terms  accommodate learnable biases for all variables, except for the generator,
        which is provided with a g0 estimate from the network
        
        optional arguments:
          batch_size (int) : batch dimension. If None, use self.batch_size.
        
        '''
        super(MomentLFADS, self).initialize_variables(batch_size=batch_size)
        if self.u_dim > 0:
            self.econ_c_1tT = torch.zeros((self.T, batch_size, self.c_encoder_dim*2)).to(self.device)
        self._rates = torch.zeros(self.T + self.kernel_dim - 1, batch_size, self.inputs_dim).to(self.device)     # Poisson process rate function
        
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
        
    def encode(self, x):
        '''
        encode(x)
        
        Function to encode the data with the forward and backward encoders.
        
        Arguments:
          - x (torch.Tensor): Single-trial data of size batch size x time-steps x input dimension
        '''
        
        for t in range(self.T):

            # generator encoders
            self.efgen_g0 = torch.clamp(self.gru_Egen_g0_forward(x[t-1], self.efgen_g0), min= -self.clip_val, max= self.clip_val)
            self.ebgen_g0 = torch.clamp(self.gru_Egen_g0_backward(x[-t], self.ebgen_g0), min= -self.clip_val, max= self.clip_val)

            if self.u_dim > 0:
                # controller encoders
                self.efcon_c  = torch.clamp(self.gru_Econ_c_forward(x[t-1], self.efcon_c), min=-self.clip_val, max=self.clip_val)
                self.ebcon_c_ = torch.clamp(self.gru_Econ_c_backward(x[-t], self.ebcon_c), min=-self.clip_val, max=self.clip_val)
                self.econ_c_1tT[t, :, :self.c_encoder_dim] = self.efcon_c.clone()
                self.econ_c_1tT[-(t+1), :, self.c_encoder_dim:] = self.ebcon_c.clone()

        # Concatenate efgen_T and ebgen_1 for generator initial condition sampling
        egen_g0 = torch.cat((self.efgen_g0, self.ebgen_g0), dim=1)

        # Dropout the generator encoder output
        if self.keep_prob < 1.0:
            egen_g0 = self.dropout(egen_g0)

        # Sample initial conditions for generator from g0 posterior distribution
        self.g0_mean   = self.fc_g0mean(egen_g0)
        self.g0_logvar = torch.clamp(self.fc_g0logvar(egen_g0), min=log(self.g0_post_var_min), max=log(self.g0_post_var_max))
        self.g0        = Variable(torch.randn(self.batch_size, self.g0_dim).to(self.device))*torch.exp(0.5*self.g0_logvar)\
                         + self.g0_mean

        self.g      = self.fc_icgen(self.g0)

        # Dropout some of the generator state for sending to factors, but keep generator state intact for feeding back to generator
        if self.keep_prob < 1.0:
            self.g_do   = self.dropout(self.g)
        
        # Initialise factors
        self.f         = self.fc_factors(self.g_do)
        
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
        
    def generate(self, x):
        '''
        generate(x)
        
        Generates the rates using the controller encoder outputs and the sampled initial conditions for
        generator.
        
        Arguments:
        - x (torch.Tensor): Single-trial data. Tensor of size batch_size x time_steps x input_size
        '''
        
        if self.infer_padding:
            for step in range(self.kernel_dim-1):
                # Update generator
                self.g = torch.clamp(self.gru_generator(None, self.g), min=-self.clip_val, max=self.clip_val)

                # Dropout on generator output, but don't overwrite generator state sent back to generator at next time step
                if self.keep_prob < 1.0:
                    self.g_do = self.dropout(self.g)

                # Generate factors from generator state
                self.f = self.fc_factors(self.g_do)

                # Generate rates from factor state
                self.r = torch.exp(self.fc_logrates(self.f))
                
                # Store in _rates for later convolution
                self._rates[step]  = self.r.clone()
        else:
            step = 0
        
        # Initialise reconstruction loss
        self.recon_loss = 0
        # Initialize KL loss and compute KL loss for Q(g0|x)
        self.compute_kl_loss_one_step(-1)
        for t in range(self.T):
            self.generate_one_step(t)
            self._rates[step+t] = self.r.clone()
            self.compute_kl_loss_one_step(t)
        
        # Apply 1-D convolution to rates to calculate moments of fluorescence
        self.fluor_moments = self.conv_fluormoments(self._rates.permute(1, 0, 2)).permute(2, 1, 0, 3)
        self.recon_loss = self.recon_loss - self.logLikelihood(x, self.fluor_moments,
                                                               self.cost_weights['ew']['weight'],
                                                               self.clip_moments)/self.batch_size
        
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
        
    def reconstruct(self, x):
        '''
        reconstruct(x)
        
        Runs a forward pass through the network, and outputs reconstruction of data x. History is not tracked.
        
        Arguments:
          - x (torch.Tensor): Single-trial data. Tensor of size batch size x time-steps x input dimensions
          
        Returns:
          - x_recon (torch.Tensor): Reconstruction. Tensor of size batch size x time-steps x input dimensions
        '''
        self.forward_no_grad(x)
        return self.fluor_moments[:, 0].detach().mean(dim=1).cpu().numpy()
    
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def _start_epoch(self):
        '''
        _start_epoch()
        
        Routine for starting training epoch
        '''
        
        if self.epochs == 1:
            for g in self.optimizer.param_groups:
                g['lr'] = self.learning_rate
                
        super(MomentLFADS, self)._start_epoch()
        
    #------------------------------------------------------------------------------
    #------------------------------------------------------------------------------
    
    def _normalize_weights(self):
        '''
        _normalize_weights()
        
        Weight adjustments after optimizer step
        '''
        super(MomentLFADS, self)._normalize_weights()
        self.conv_fluormoments.weight.data[:2] = self.conv_fluormoments.weight.data[:2].clamp(min=0.0)
